{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta as rd\n",
    "import time\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stocks data csv read\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# s&p data csv read\n",
    "df_sp = pd.read_csv('sp500.csv')\n",
    "df_sp = df_sp.set_index('Date')\n",
    "\n",
    "# stocks data csv read for partial replication\n",
    "df_reduce = pd.read_csv('data.csv')\n",
    "df_reduce = df_reduce.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_slicer(df, start, duration, rebalancing_period=0):\n",
    "    '''\n",
    "    this function is used to slice out specific section of the data\n",
    "    '''\n",
    "    start = str(datetime.strptime(start, '%Y-%m-%d').date() + rd(months=rebalancing_period))\n",
    "    end = str(datetime.strptime(start, '%Y-%m-%d').date() + rd(months=duration) - rd(days=1))\n",
    "    return df.loc[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(df):\n",
    "    '''\n",
    "    this function gets the dataframe as input, processes it, and ouputs the cumulative change of the stocks\n",
    "    that is used as input for training the model.\n",
    "    '''\n",
    "    df = df.pct_change()\n",
    "    df = df.tail(-1)\n",
    "    df = df + 1\n",
    "    df = df.cumprod()\n",
    "    df = df - 1\n",
    "    df = df.iloc[-1,:]\n",
    "    df = df.to_numpy()\n",
    "    df = torch.from_numpy(df).type(torch.Tensor)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_change(df):\n",
    "    '''\n",
    "    this function calculate the daily change of stocks included in the dataframe.\n",
    "    '''\n",
    "    df = df.pct_change()\n",
    "    df = df.tail(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_return(df):\n",
    "    '''\n",
    "    this function calculate the daily return of stocks included in the dataframe, note that \n",
    "    daily return is equal to daily change + 1\n",
    "    '''\n",
    "    df = df.pct_change()\n",
    "    df = df.tail(-1)\n",
    "    df = df + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(df):\n",
    "    '''\n",
    "    this function is just being used for extracting the stocks symbols\n",
    "    '''\n",
    "    df = df.pct_change()\n",
    "    df = df.tail(-1)\n",
    "    df = df + 1\n",
    "    df = df.cumprod()\n",
    "    df = df - 1\n",
    "    df = df.iloc[-1,:]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing stocks symbols\n",
    "stocks_index = index_finder(df).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow nnf biuld\n",
    "class shallow_NNF(nn.Module):\n",
    "    '''\n",
    "    this class is used to train the data with Shallow NNF model, consisted of 2 fully connected layers, \n",
    "    a relU activation function in between and a softmax layer output that is translated into stock weights in portfolio.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_size, num_classes):\n",
    "        super(shallow_NNF, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size) # fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # fully connected layer\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.softmax(self.fc2(out))\n",
    "        weights = out\n",
    "        cumulative_change = sum(out * x)\n",
    "        return cumulative_change, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "shallow nnf partial biuld which is the same as original shallow nnf\n",
    "this class helps us to use the full replication training to find the best companies to invest\n",
    "and then find the optimal wieghts with the partial model\n",
    "'''\n",
    "class shallow_NNF_partial(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, num_classes):\n",
    "        super(shallow_NNF_partial, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.softmax(self.fc2(out))\n",
    "        weights = out\n",
    "        cumulative_change = sum(out * x)\n",
    "        return cumulative_change, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep nnf build\n",
    "class deep_NNF(nn.Module):\n",
    "    '''\n",
    "    this class is used to train the data with Deep NNF model, consisted of 6 fully connected layers, \n",
    "    relU activation functions in between and a softmax layer output that is translated into stock weights in portfolio.\n",
    "    dropout is also included in deep NNF model.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_size1, hidden_size2, hidden_size3,\n",
    "                 hidden_size4, hidden_size5, num_classes, dropout_p = 0.2):\n",
    "        super(deep_NNF, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size1) # fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2) # fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3) # fully connected layer\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4) # fully connected layer\n",
    "        self.fc5 = nn.Linear(hidden_size4, hidden_size5) # fully connected layer\n",
    "        self.fc6 = nn.Linear(hidden_size5, num_classes) # fully connected layer\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()\n",
    "        self.fc3.reset_parameters()\n",
    "        self.fc4.reset_parameters()\n",
    "        self.fc5.reset_parameters()\n",
    "        self.fc6.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc5(out))\n",
    "        out = self.softmax(self.fc6(out))\n",
    "        weights = out\n",
    "        cumulative_change = sum(out * x)\n",
    "        return cumulative_change, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "deep nnf partial biuld which is the same as original deep nnf\n",
    "this class helps us to use the full replication training to find the best companies to invest\n",
    "and then find the optimal wieghts with the partial model\n",
    "'''\n",
    "class deep_NNF_partial(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size1, hidden_size2, hidden_size3,\n",
    "                 hidden_size4, hidden_size5, num_classes, dropout_p = 0.2):\n",
    "        super(deep_NNF_partial, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.fc4 = nn.Linear(hidden_size3, hidden_size4)\n",
    "        self.fc5 = nn.Linear(hidden_size4, hidden_size5)\n",
    "        self.fc6 = nn.Linear(hidden_size5, num_classes)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.reset_parameters()\n",
    "        self.fc2.reset_parameters()\n",
    "        self.fc3.reset_parameters()\n",
    "        self.fc4.reset_parameters()\n",
    "        self.fc5.reset_parameters()\n",
    "        self.fc6.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc5(out))\n",
    "        out = self.softmax(self.fc6(out))\n",
    "        weights = out\n",
    "        cumulative_change = sum(out * x)\n",
    "        return cumulative_change, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/N model build\n",
    "class equal_w_model():\n",
    "    '''\n",
    "    this class is used to construct a portfolio with equal weights.\n",
    "    '''\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.performance()\n",
    "        \n",
    "    def performance(self):\n",
    "        self.df = np.array(self.df)\n",
    "        weights = np.ones((len(self.df), 1)) * (1/len(self.df))\n",
    "        cumulative_change = sum(np.multiply(weights, self.df.reshape(-1,1)))\n",
    "        return cumulative_change, weights.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebalancing period = one or three months\n",
    "rbp = 1\n",
    "\n",
    "# number of companies in the partial portfolio\n",
    "partial_num = 50\n",
    "\n",
    "# epochs\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow_nnf hyperparameters\n",
    "input_dim = 471\n",
    "hidden_size = 471\n",
    "num_classes = 471\n",
    "lr = 1e-3 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow nnf tune\n",
    "'''\n",
    "loss function is set to MSE and Adam optimizer is used in this model.\n",
    "'''\n",
    "shallow_NNF = shallow_NNF(input_dim=input_dim, hidden_size=hidden_size, num_classes=num_classes)\n",
    "shallow_NNF_loss_fun = torch.nn.MSELoss(reduction='mean')\n",
    "shallow_NNF_optimizer = torch.optim.Adam(shallow_NNF.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow nnf partial tune\n",
    "'''\n",
    "loss function is set to MSE and Adam optimizer is used in this model.\n",
    "'''\n",
    "shallow_NNF_partial = shallow_NNF_partial(input_dim=partial_num, hidden_size=hidden_size, num_classes=partial_num)\n",
    "shallow_NNF_partial_loss_fun = torch.nn.MSELoss(reduction='mean')\n",
    "shallow_NNF_partial_optimizer = torch.optim.Adam(shallow_NNF_partial.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep_nnf hyperparameters\n",
    "input_dim = 471\n",
    "hidden_size1 = 471\n",
    "hidden_size2 = 471\n",
    "hidden_size3 = 471\n",
    "hidden_size4 = 471\n",
    "hidden_size5 = 471\n",
    "num_classes = 471\n",
    "lr = 1e-7 # learning rate\n",
    "# probability of a neuron being shutdown that shuffles every epoch minimizing the overfit phenomenon\n",
    "dropout_p = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep nnf tune\n",
    "'''\n",
    "like in shallow NNF, loss function is set to MSE and Adam optimizer is used.\n",
    "'''\n",
    "deep_NNF = deep_NNF(input_dim=input_dim, hidden_size1=hidden_size1, hidden_size2=hidden_size2, \n",
    "                    hidden_size3=hidden_size3, hidden_size4=hidden_size4, hidden_size5=hidden_size5,\n",
    "                    num_classes=num_classes)\n",
    "deep_NNF_loss_fun = torch.nn.MSELoss(reduction='mean')\n",
    "deep_NNF_optimizer = torch.optim.Adam(deep_NNF.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep nnf partial tune\n",
    "'''\n",
    "like in shallow NNF, loss function is set to MSE and Adam optimizer is used.\n",
    "'''\n",
    "deep_NNF_partial = deep_NNF_partial(input_dim=partial_num, hidden_size1=hidden_size1, hidden_size2=hidden_size2, \n",
    "                    hidden_size3=hidden_size3, hidden_size4=hidden_size4, hidden_size5=hidden_size5,\n",
    "                    num_classes=partial_num)\n",
    "deep_NNF_partial_loss_fun = torch.nn.MSELoss(reduction='mean')\n",
    "deep_NNF_partial_optimizer = torch.optim.Adam(deep_NNF_partial.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "def RMSE(x, y, weights):\n",
    "    '''\n",
    "    this function calculates the root mean squere error of constructed portfollio and benchmark index \n",
    "    that is used for evaluating trained models.\n",
    "    '''\n",
    "    temp = 0\n",
    "    for i in range(len(x)):\n",
    "        temp += (sum(x.iloc[i] * weights) - y.iloc[i]) ** 2\n",
    "    return math.sqrt(temp/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN\n",
    "def MEAN(x, weights):\n",
    "    '''\n",
    "    this function calculates the mean return of the constructed portfolio during the given period.\n",
    "    '''\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(sum(x.iloc[i] * weights))\n",
    "    temp = np.array(temp)\n",
    "    return temp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volatility\n",
    "def VOL(x, weights):\n",
    "    '''\n",
    "    this function calculates the volatility of the constructed portfolio during the given period.\n",
    "    '''\n",
    "    temp = []\n",
    "    for i in range(len(x)):\n",
    "        temp.append(sum(x.iloc[i] * weights))\n",
    "    temp = np.array(temp)\n",
    "    return temp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(df, x_test, model, i, temp):   \n",
    "    '''\n",
    "    this function outputs the cumulative return of the portfolio test dataset of the given dataframe\n",
    "    ''' \n",
    "    x_return = date_slicer(df, '2018-01-01', 1, i)\n",
    "    x_return =  x_return.pct_change()\n",
    "    x_return =  x_return.tail(-1)\n",
    "    x_return =  x_return + 1\n",
    "    x_return =  x_return.cumprod()\n",
    "    \n",
    "    if model == equal_w_model:\n",
    "        weights = model(x_test).performance()[1]\n",
    "    else:\n",
    "        weights = np.array(model(x_test)[1].detach())\n",
    "    \n",
    "    for i in range(len(x_return)):\n",
    "        temp.append(sum(x_return.iloc[i] * weights))\n",
    "    temp = np.array(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_return(df_sp, i, temp):\n",
    "    '''\n",
    "    this function outputs the cumulative return of the benchmark index test dataset of the given dataframe\n",
    "    '''\n",
    "    y_return = date_slicer(df_sp, '2018-01-01', 1, i)\n",
    "    y_return = y_return.pct_change()\n",
    "    y_return = y_return.tail(-1)\n",
    "    y_return = y_return + 1\n",
    "    y_return = y_return.cumprod()\n",
    "    \n",
    "    for i in range(len(y_return)):\n",
    "        temp.append(sum(y_return.iloc[i]))\n",
    "    temp = np.array(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fun(x_valid, i, model):\n",
    "    '''\n",
    "    this function gets validation dataset, model and rebalaning period as input, then outputs the RMSE of given dataset.\n",
    "    '''\n",
    "    x_change = daily_change(date_slicer(df_reduce, '2017-07-01', 6, i))\n",
    "    y_change = daily_change(date_slicer(df_sp, '2017-07-01', 6, i))\n",
    "    # x_return = daily_return(date_slicer(df, '2017-07-01', 6, i))\n",
    "    # y_return = daily_return(date_slicer(df_sp, '2017-07-01', 6, i))\n",
    "    \n",
    "    if model == equal_w_model:\n",
    "        weights = model(x_valid).performance()[1]\n",
    "    else:\n",
    "        weights = np.array(model(x_valid)[1].detach())\n",
    "    \n",
    "    valid_rmse = RMSE(x_change, y_change, weights)\n",
    "    # valid_mean = MEAN(x_return, weights)\n",
    "    # valid_vol  = VOL(x_return, weights)\n",
    "    \n",
    "    print(f'Validation RMSE: {valid_rmse}')\n",
    "    # print(f'Validation MEAN: {valid_mean}')\n",
    "    # print(f'Validation VOL: {valid_vol}')\n",
    "    \n",
    "    return valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fun(x_test, i, model):\n",
    "    '''\n",
    "    this function gets test dataset, model and rebalaning period as input, then outputs the RMSE, Mean and volatility \n",
    "    of the given dataset.\n",
    "    '''\n",
    "    x_change = daily_change(date_slicer(df_reduce, '2018-01-01', 6, i))\n",
    "    y_change = daily_change(date_slicer(df_sp, '2018-01-01', 6, i))\n",
    "    x_return = daily_return(date_slicer(df_reduce, '2018-01-01', 6, i))\n",
    "    y_return = daily_return(date_slicer(df_sp, '2018-01-01', 6, i))\n",
    "    \n",
    "    if model == equal_w_model:\n",
    "        weights = model(x_test).performance()[1]\n",
    "    else:\n",
    "        weights = np.array(model(x_test)[1].detach())\n",
    "    \n",
    "    test_rmse = RMSE(x_change, y_change, weights)\n",
    "    test_mean = MEAN(x_return, weights)\n",
    "    test_vol  = VOL(x_return, weights)\n",
    "    test_dic = {'RMSE': test_rmse, 'MEAN': test_mean, 'VOL': test_vol}\n",
    "    \n",
    "    print(f'Test RMSE: {test_rmse}')\n",
    "    print(f'Test MEAN: {test_mean}')\n",
    "    print(f'Test VOL: {test_vol}')\n",
    "    \n",
    "    return test_dic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Deep NNF Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep nnf training function\n",
    "'''\n",
    "this function is used to train the model using x_train & y_train given to it, printing MSE of trained model in first and last\n",
    " epich and also printing train time of the model\n",
    "'''\n",
    "def train_deep_nnf(x_train, y_train, i):\n",
    "    start_time_deep_nnf = time.time()\n",
    "    print(f'\\nDeep NNF Training & Results for model {(i/rbp)+1} (Full replication) :')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_train_pred = deep_NNF(x_train)[0]\n",
    "        loss_deep_nnf = deep_NNF_loss_fun(y_train_pred, y_train)\n",
    "        if epoch == 0 or epoch == num_epochs-1:\n",
    "            weights = np.array(deep_NNF(x_train)[1].detach())\n",
    "            print(f'Epoch {epoch+1} of {num_epochs} | MSE: {loss_deep_nnf.item()}')\n",
    "        deep_NNF_optimizer.zero_grad()\n",
    "        loss_deep_nnf.backward()\n",
    "        deep_NNF_optimizer.step()\n",
    "        \n",
    "    training_time = format(time.time()-start_time_deep_nnf, '0.2f')\n",
    "    print(f'Training time: {training_time}')\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep nnf partial training function\n",
    "def train_deep_nnf_partial(x_train, y_train, i):    \n",
    "    start_time_deep_nnf = time.time()\n",
    "    print(f'\\nDeep NNF Training & Results for model {(i/rbp)+1} (Partial replication):')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_train_pred = deep_NNF_partial(x_train)[0]\n",
    "        loss_deep_nnf = deep_NNF_partial_loss_fun(y_train_pred, y_train)\n",
    "        if epoch == 0 or epoch == num_epochs-1:\n",
    "            print(f'Epoch {epoch+1} of {num_epochs} | MSE: {loss_deep_nnf.item()}')\n",
    "        deep_NNF_partial_optimizer.zero_grad()\n",
    "        loss_deep_nnf.backward()\n",
    "        deep_NNF_partial_optimizer.step()\n",
    "        \n",
    "    training_time = format(time.time()-start_time_deep_nnf, '0.2f')\n",
    "    print(f'Training time: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial(x_train, x_valid, x_test, weights, stocks_index, num = partial_num):\n",
    "    df_partial = pd.DataFrame({'x_train': x_train, 'x_valid': x_valid, 'x_test': x_test,\n",
    "                               'weights': weights}, index = stocks_index)\n",
    "    df_partial = df_partial.sort_values(by = ['weights'])\n",
    "    out_index = df_partial.index[num:]\n",
    "    df_partial = df_partial.iloc[:num]\n",
    "    \n",
    "    x_train = df_partial['x_train'].to_numpy()\n",
    "    x_valid = df_partial['x_valid'].to_numpy()\n",
    "    x_test = df_partial['x_test'].to_numpy()\n",
    "    \n",
    "    x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "    x_valid = torch.from_numpy(x_valid).type(torch.Tensor)\n",
    "    x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "    \n",
    "    return x_train, x_valid, x_test, out_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deep NNF Training & Results for model 1.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.045260295271873474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamidrezarahimzadeh/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 of 100 | MSE: 0.04515916109085083\n",
      "Training time: 0.96\n",
      "\n",
      "Deep NNF Training & Results for model 1.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.05589902028441429\n",
      "Epoch 100 of 100 | MSE: 0.05538555234670639\n",
      "Training time: 0.47\n",
      "Validation RMSE: 0.0020451043470705796\n",
      "Test RMSE: 0.002462575698868579\n",
      "Test MEAN: 1.0001573203256373\n",
      "Test VOL: 0.009617330350154527\n",
      "\n",
      "Deep NNF Training & Results for model 2.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.05435974523425102\n",
      "Epoch 100 of 100 | MSE: 0.05418667197227478\n",
      "Training time: 0.90\n",
      "\n",
      "Deep NNF Training & Results for model 2.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0016391891986131668\n",
      "Epoch 100 of 100 | MSE: 0.001591934240423143\n",
      "Training time: 0.49\n",
      "Validation RMSE: 0.0025480420808377587\n",
      "Test RMSE: 0.0025893886158141685\n",
      "Test MEAN: 1.0003575306591284\n",
      "Test VOL: 0.009695955940464273\n",
      "\n",
      "Deep NNF Training & Results for model 3.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04551883786916733\n",
      "Epoch 100 of 100 | MSE: 0.04552730917930603\n",
      "Training time: 0.92\n",
      "\n",
      "Deep NNF Training & Results for model 3.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.02414204552769661\n",
      "Epoch 100 of 100 | MSE: 0.02426532842218876\n",
      "Training time: 0.46\n",
      "Validation RMSE: 0.002923053788620484\n",
      "Test RMSE: 0.002858195452485741\n",
      "Test MEAN: 1.000759187240896\n",
      "Test VOL: 0.007225579811543839\n",
      "\n",
      "Deep NNF Training & Results for model 4.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.059426385909318924\n",
      "Epoch 100 of 100 | MSE: 0.05927116051316261\n",
      "Training time: 1.04\n",
      "\n",
      "Deep NNF Training & Results for model 4.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.016249418258666992\n",
      "Epoch 100 of 100 | MSE: 0.016217295080423355\n",
      "Training time: 0.48\n",
      "Validation RMSE: 0.0024652889149516465\n",
      "Test RMSE: 0.002198461141717616\n",
      "Test MEAN: 1.0010317791632146\n",
      "Test VOL: 0.005988218493959352\n",
      "\n",
      "Deep NNF Training & Results for model 5.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04667492210865021\n",
      "Epoch 100 of 100 | MSE: 0.04675242677330971\n",
      "Training time: 0.96\n",
      "\n",
      "Deep NNF Training & Results for model 5.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.051467008888721466\n",
      "Epoch 100 of 100 | MSE: 0.05145051330327988\n",
      "Training time: 0.79\n",
      "Validation RMSE: 0.0028327200221685512\n",
      "Test RMSE: 0.003101635903715933\n",
      "Test MEAN: 1.0002205049763733\n",
      "Test VOL: 0.006981733051419369\n",
      "\n",
      "Deep NNF Training & Results for model 6.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04949358478188515\n",
      "Epoch 100 of 100 | MSE: 0.04940737783908844\n",
      "Training time: 1.62\n",
      "\n",
      "Deep NNF Training & Results for model 6.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.08701343834400177\n",
      "Epoch 100 of 100 | MSE: 0.08714472502470016\n",
      "Training time: 0.74\n",
      "Validation RMSE: 0.002493391240723963\n",
      "Test RMSE: 0.0029650319528594323\n",
      "Test MEAN: 1.0004510302025238\n",
      "Test VOL: 0.008281727372157367\n",
      "\n",
      "Deep NNF Training & Results for model 7.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03939768671989441\n",
      "Epoch 100 of 100 | MSE: 0.03924914821982384\n",
      "Training time: 1.31\n",
      "\n",
      "Deep NNF Training & Results for model 7.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.042353685945272446\n",
      "Epoch 100 of 100 | MSE: 0.04231002926826477\n",
      "Training time: 0.56\n",
      "Validation RMSE: 0.002550894954789689\n",
      "Test RMSE: 0.0028491739804927356\n",
      "Test MEAN: 0.9996863721300608\n",
      "Test VOL: 0.009958628109689983\n",
      "\n",
      "Deep NNF Training & Results for model 8.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03763618692755699\n",
      "Epoch 100 of 100 | MSE: 0.03786296024918556\n",
      "Training time: 1.11\n",
      "\n",
      "Deep NNF Training & Results for model 8.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.007970957085490227\n",
      "Epoch 100 of 100 | MSE: 0.007894802838563919\n",
      "Training time: 0.55\n",
      "Validation RMSE: 0.0033083357325452505\n",
      "Test RMSE: 0.004428925689107601\n",
      "Test MEAN: 1.0001131991593724\n",
      "Test VOL: 0.009521411330457672\n",
      "\n",
      "Deep NNF Training & Results for model 9.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.026833124458789825\n",
      "Epoch 100 of 100 | MSE: 0.026738548651337624\n",
      "Training time: 1.28\n",
      "\n",
      "Deep NNF Training & Results for model 9.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.07614602893590927\n",
      "Epoch 100 of 100 | MSE: 0.07590893656015396\n",
      "Training time: 0.59\n",
      "Validation RMSE: 0.002283715130866073\n",
      "Test RMSE: 0.002885909425589797\n",
      "Test MEAN: 0.999936227075912\n",
      "Test VOL: 0.012006394244733149\n",
      "\n",
      "Deep NNF Training & Results for model 10.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03137144073843956\n",
      "Epoch 100 of 100 | MSE: 0.03145333379507065\n",
      "Training time: 1.22\n",
      "\n",
      "Deep NNF Training & Results for model 10.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0033317331690341234\n",
      "Epoch 100 of 100 | MSE: 0.003311564913019538\n",
      "Training time: 0.65\n",
      "Validation RMSE: 0.0026367585738550237\n",
      "Test RMSE: 0.0034137600406424543\n",
      "Test MEAN: 1.0000025685069023\n",
      "Test VOL: 0.011427253929921444\n",
      "\n",
      "Deep NNF Training & Results for model 11.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03541478514671326\n",
      "Epoch 100 of 100 | MSE: 0.035390403121709824\n",
      "Training time: 1.42\n",
      "\n",
      "Deep NNF Training & Results for model 11.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.024918541312217712\n",
      "Epoch 100 of 100 | MSE: 0.024926474317908287\n",
      "Training time: 0.66\n",
      "Validation RMSE: 0.002596943243243733\n",
      "Test RMSE: 0.0026398369295800627\n",
      "Test MEAN: 1.0008397364016377\n",
      "Test VOL: 0.010558723254760044\n",
      "\n",
      "Deep NNF Training & Results for model 12.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03527733311057091\n",
      "Epoch 100 of 100 | MSE: 0.035186946392059326\n",
      "Training time: 1.19\n",
      "\n",
      "Deep NNF Training & Results for model 12.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.004617584403604269\n",
      "Epoch 100 of 100 | MSE: 0.004687614738941193\n",
      "Training time: 0.77\n",
      "Validation RMSE: 0.003294609264117122\n",
      "Test RMSE: 0.0026648764136803886\n",
      "Test MEAN: 0.9999398571469593\n",
      "Test VOL: 0.010075865547079003\n",
      "\n",
      "Deep NNF Training & Results for model 13.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.034741949290037155\n",
      "Epoch 100 of 100 | MSE: 0.034451525658369064\n",
      "Training time: 1.40\n",
      "\n",
      "Deep NNF Training & Results for model 13.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.04531829431653023\n",
      "Epoch 100 of 100 | MSE: 0.045552220195531845\n",
      "Training time: 0.85\n",
      "Validation RMSE: 0.0030233791426538792\n",
      "Test RMSE: 0.0025330098252848406\n",
      "Test MEAN: 1.0014928823287281\n",
      "Test VOL: 0.008067375567432238\n",
      "\n",
      "Deep NNF Training & Results for model 14.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.03397611528635025\n",
      "Epoch 100 of 100 | MSE: 0.033912599086761475\n",
      "Training time: 1.36\n",
      "\n",
      "Deep NNF Training & Results for model 14.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.010381213389337063\n",
      "Epoch 100 of 100 | MSE: 0.010225036181509495\n",
      "Training time: 0.85\n",
      "Validation RMSE: 0.0034083194051702805\n",
      "Test RMSE: 0.002673599193831319\n",
      "Test MEAN: 1.0009583034183458\n",
      "Test VOL: 0.006617191491321232\n",
      "\n",
      "Deep NNF Training & Results for model 15.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.036085255444049835\n",
      "Epoch 100 of 100 | MSE: 0.036091845482587814\n",
      "Training time: 1.32\n",
      "\n",
      "Deep NNF Training & Results for model 15.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.021498309448361397\n",
      "Epoch 100 of 100 | MSE: 0.020983858034014702\n",
      "Training time: 0.85\n",
      "Validation RMSE: 0.0034184042927953447\n",
      "Test RMSE: 0.0027609421378807304\n",
      "Test MEAN: 1.0004165150319386\n",
      "Test VOL: 0.009116033652320556\n",
      "\n",
      "Deep NNF Training & Results for model 16.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04008658975362778\n",
      "Epoch 100 of 100 | MSE: 0.039457276463508606\n",
      "Training time: 1.40\n",
      "\n",
      "Deep NNF Training & Results for model 16.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.009292619302868843\n",
      "Epoch 100 of 100 | MSE: 0.009179167449474335\n",
      "Training time: 0.62\n",
      "Validation RMSE: 0.0030157306614530363\n",
      "Test RMSE: 0.002402360149038392\n",
      "Test MEAN: 1.0006813784276058\n",
      "Test VOL: 0.007643142999118569\n",
      "\n",
      "Deep NNF Training & Results for model 17.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.022619124501943588\n",
      "Epoch 100 of 100 | MSE: 0.02256985567510128\n",
      "Training time: 1.28\n",
      "\n",
      "Deep NNF Training & Results for model 17.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.017258964478969574\n",
      "Epoch 100 of 100 | MSE: 0.01713772676885128\n",
      "Training time: 0.67\n",
      "Validation RMSE: 0.0029370184773013603\n",
      "Test RMSE: 0.0028642666192571797\n",
      "Test MEAN: 1.0005389167655863\n",
      "Test VOL: 0.009027043296081852\n",
      "\n",
      "Deep NNF Training & Results for model 18.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.02287314645946026\n",
      "Epoch 100 of 100 | MSE: 0.022814635187387466\n",
      "Training time: 1.20\n",
      "\n",
      "Deep NNF Training & Results for model 18.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.04276115447282791\n",
      "Epoch 100 of 100 | MSE: 0.04250258952379227\n",
      "Training time: 0.69\n",
      "Validation RMSE: 0.002973974282681554\n",
      "Test RMSE: 0.002328101680612061\n",
      "Test MEAN: 1.0011375959811324\n",
      "Test VOL: 0.008038357227128087\n",
      "\n",
      "Deep NNF Training & Results for model 19.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.020608851686120033\n",
      "Epoch 100 of 100 | MSE: 0.02051912061870098\n",
      "Training time: 1.21\n",
      "\n",
      "Deep NNF Training & Results for model 19.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.008688164874911308\n",
      "Epoch 100 of 100 | MSE: 0.008670744486153126\n",
      "Training time: 0.70\n",
      "Validation RMSE: 0.0030932692168549594\n",
      "Test RMSE: 0.0022815997455477898\n",
      "Test MEAN: 1.0006264076885227\n",
      "Test VOL: 0.008137564062315084\n",
      "\n",
      "Deep NNF Training & Results for model 20.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04265021160244942\n",
      "Epoch 100 of 100 | MSE: 0.04248670116066933\n",
      "Training time: 1.31\n",
      "\n",
      "Deep NNF Training & Results for model 20.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.04342667758464813\n",
      "Epoch 100 of 100 | MSE: 0.04357222467660904\n",
      "Training time: 0.60\n",
      "Validation RMSE: 0.002844762790616034\n",
      "Test RMSE: 0.002344640942717199\n",
      "Test MEAN: 1.000804403936032\n",
      "Test VOL: 0.007754982963941262\n",
      "\n",
      "Deep NNF Training & Results for model 21.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.04115397855639458\n",
      "Epoch 100 of 100 | MSE: 0.04124290123581886\n",
      "Training time: 1.19\n",
      "\n",
      "Deep NNF Training & Results for model 21.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.1269897222518921\n",
      "Epoch 100 of 100 | MSE: 0.12569744884967804\n",
      "Training time: 0.60\n",
      "Validation RMSE: 0.002512356531183999\n",
      "Test RMSE: 0.0022604704555550933\n",
      "Test MEAN: 1.0013279415186611\n",
      "Test VOL: 0.00636362553156093\n",
      "\n",
      "Deep NNF Training & Results for model 22.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.028837550431489944\n",
      "Epoch 100 of 100 | MSE: 0.028809964656829834\n",
      "Training time: 1.21\n",
      "\n",
      "Deep NNF Training & Results for model 22.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.024526480585336685\n",
      "Epoch 100 of 100 | MSE: 0.024817459285259247\n",
      "Training time: 0.61\n",
      "Validation RMSE: 0.002226699956974882\n",
      "Test RMSE: 0.001953814050711308\n",
      "Test MEAN: 1.0018901696826157\n",
      "Test VOL: 0.00627016674709143\n",
      "\n",
      "Deep NNF Training & Results for model 23.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.0255706999450922\n",
      "Epoch 100 of 100 | MSE: 0.02554156258702278\n",
      "Training time: 1.17\n",
      "\n",
      "Deep NNF Training & Results for model 23.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.08187464624643326\n",
      "Epoch 100 of 100 | MSE: 0.08279331028461456\n",
      "Training time: 0.60\n",
      "Validation RMSE: 0.0029867690307189133\n",
      "Test RMSE: 0.0023040906373154534\n",
      "Test MEAN: 1.001173503634211\n",
      "Test VOL: 0.004752858122279884\n",
      "\n",
      "Deep NNF Training & Results for model 24.0 (Full replication) :\n",
      "Epoch 1 of 100 | MSE: 0.017157161608338356\n",
      "Epoch 100 of 100 | MSE: 0.017162276431918144\n",
      "Training time: 1.17\n",
      "\n",
      "Deep NNF Training & Results for model 24.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.1441374272108078\n",
      "Epoch 100 of 100 | MSE: 0.1429670751094818\n",
      "Training time: 0.60\n",
      "Validation RMSE: 0.0027802026093495594\n",
      "Test RMSE: 0.001695446437423981\n",
      "Test MEAN: 1.001789310766005\n",
      "Test VOL: 0.004767741683221085\n",
      "\n",
      "Min Valid RMSE is: 0.0020451043470705796 for model i = 1\n",
      "Selected Model Test Results are:\n",
      "RMSE = 0.002462575698868579\n",
      "MEAN = 1.0001573203256373\n",
      "VOL = 0.009617330350154527\n"
     ]
    }
   ],
   "source": [
    "# deep nnf\n",
    "'''\n",
    "in this cell,firstly, train, validation and test datasets are sliced in each loop. then deep NNf outputs the best stocks with \n",
    "full replication and then we use the specific stocks to train the model again and get the optimal weights (each loop)\n",
    "then best model will be chosen. Also RMSE, Mean and volatility of all models and then the best model is printed.\n",
    "'''\n",
    "deep_nnf_valid_rmse_list = []\n",
    "deep_nnf_test_results = []\n",
    "out_index_history = []\n",
    "deep_nnf_test_plot = [] # storing the deep model test data return for plotting later on\n",
    "index_test_plot = [] # storing the index test data return for plotting later on\n",
    "\n",
    "for i in range(int(24/rbp)):\n",
    "    df_reduce = df.copy()    \n",
    "    x_train = data_process(date_slicer(df, '2014-07-01', 36, i*rbp))\n",
    "    y_train = data_process(date_slicer(df_sp, '2014-07-01', 36, i*rbp))\n",
    "    x_valid = data_process(date_slicer(df, '2017-07-01', 6, i*rbp))\n",
    "    y_valid = data_process(date_slicer(df_sp, '2017-07-01', 6, i*rbp))\n",
    "    x_test = data_process(date_slicer(df, '2018-01-01', 1, i*rbp))\n",
    "    y_test = data_process(date_slicer(df_sp, '2018-01-01', 1, i*rbp))\n",
    "    weights = train_deep_nnf(x_train, y_train, i*rbp)\n",
    "    x_train, x_valid, x_test, out_index = partial(x_train, x_valid, x_test, weights, stocks_index, num = partial_num)\n",
    "    out_index_history.append(out_index)\n",
    "    df_reduce = df_reduce.drop(out_index, axis=1)\n",
    "    train_deep_nnf_partial(x_train, y_train, i*rbp)\n",
    "    deep_nnf_valid_rmse_list.append(valid_fun(x_valid, i*rbp, deep_NNF_partial))\n",
    "    deep_nnf_test_results.append(test_fun(x_test, i*rbp, deep_NNF_partial))\n",
    "    portfolio_return(df_reduce, x_test, deep_NNF_partial, i, deep_nnf_test_plot)\n",
    "    index_return(df_sp, i, index_test_plot)\n",
    "    deep_NNF.reset_parameters()\n",
    "    deep_NNF_partial.reset_parameters()\n",
    "\n",
    "print(f'\\nMin Valid RMSE is: {min(deep_nnf_valid_rmse_list)} for model i = {deep_nnf_valid_rmse_list.index(min(deep_nnf_valid_rmse_list))+1}')\n",
    "print('Selected Model Test Results are:')\n",
    "print('RMSE =', deep_nnf_test_results[deep_nnf_valid_rmse_list.index(min(deep_nnf_valid_rmse_list))]['RMSE'])\n",
    "print('MEAN =', deep_nnf_test_results[deep_nnf_valid_rmse_list.index(min(deep_nnf_valid_rmse_list))]['MEAN'])\n",
    "print('VOL =', deep_nnf_test_results[deep_nnf_valid_rmse_list.index(min(deep_nnf_valid_rmse_list))]['VOL'])\n",
    "\n",
    "deep_best_result_index = deep_nnf_valid_rmse_list.index(min(deep_nnf_valid_rmse_list))\n",
    "deep_nnf_test_plot = np.array(deep_nnf_test_plot).reshape(-1,1)\n",
    "index_test_plot = np.array(index_test_plot).reshape(-1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Shallow NNF Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow nnf training function\n",
    "def train_shallow_nnf(x_train, y_train, i):\n",
    "    '''\n",
    "    this function is used to train the model using x_train & y_train given to it, printing MSE of trained model in first and last\n",
    "    epoch and also printing train time of the model\n",
    "    '''\n",
    "    start_time_shallow_nnf = time.time()\n",
    "    print(f'\\nShallow NNF Training & Results for model {(i/rbp)+1}:')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_train_pred = shallow_NNF(x_train)[0]\n",
    "        loss_shallow_nnf = shallow_NNF_loss_fun(y_train_pred, y_train)\n",
    "        if epoch == 0 or epoch == num_epochs-1:\n",
    "            weights = np.array(deep_NNF(x_train)[1].detach())\n",
    "            print(f'Epoch {epoch+1} of {num_epochs} | MSE: {loss_shallow_nnf.item()}')\n",
    "        shallow_NNF_optimizer.zero_grad()\n",
    "        loss_shallow_nnf.backward()\n",
    "        shallow_NNF_optimizer.step()\n",
    "        \n",
    "    training_time = format(time.time()-start_time_shallow_nnf, '0.2f')\n",
    "    print(f'Training time: {training_time}')\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shallow nnf partial training function\n",
    "def train_shallow_nnf_partial(x_train, y_train, i):    \n",
    "    start_time_shallow_nnf = time.time()\n",
    "    print(f'\\nDeep NNF Training & Results for model {(i/rbp)+1} (Partial replication):')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        y_train_pred = shallow_NNF_partial(x_train)[0]\n",
    "        loss_shallow_nnf = shallow_NNF_partial_loss_fun(y_train_pred, y_train)\n",
    "        if epoch == 0 or epoch == num_epochs-1:\n",
    "            print(f'Epoch {epoch+1} of {num_epochs} | MSE: {loss_shallow_nnf.item()}')\n",
    "        shallow_NNF_partial_optimizer.zero_grad()\n",
    "        loss_shallow_nnf.backward()\n",
    "        shallow_NNF_partial_optimizer.step()\n",
    "        \n",
    "    training_time = format(time.time()-start_time_shallow_nnf, '0.2f')\n",
    "    print(f'Training time: {training_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shallow NNF Training & Results for model 1.0:\n",
      "Epoch 1 of 100 | MSE: 0.04181593656539917\n",
      "Epoch 100 of 100 | MSE: 8.58848281382052e-08\n",
      "Training time: 0.73\n",
      "\n",
      "Deep NNF Training & Results for model 1.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.03964419290423393\n",
      "Epoch 100 of 100 | MSE: 2.388990480994835e-07\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.002700058468266041\n",
      "Test RMSE: 0.002720049127052518\n",
      "Test MEAN: 0.9999396353758883\n",
      "Test VOL: 0.009935167920603755\n",
      "\n",
      "Shallow NNF Training & Results for model 2.0:\n",
      "Epoch 1 of 100 | MSE: 0.06081223860383034\n",
      "Epoch 100 of 100 | MSE: 2.3510992264164088e-08\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 2.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.05265847221016884\n",
      "Epoch 100 of 100 | MSE: 1.8916853150585666e-07\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.002879562746177191\n",
      "Test RMSE: 0.0029167965604236013\n",
      "Test MEAN: 1.0001634175719611\n",
      "Test VOL: 0.009948454045100032\n",
      "\n",
      "Shallow NNF Training & Results for model 3.0:\n",
      "Epoch 1 of 100 | MSE: 0.044627439230680466\n",
      "Epoch 100 of 100 | MSE: 1.3273881993569603e-09\n",
      "Training time: 0.71\n",
      "\n",
      "Deep NNF Training & Results for model 3.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.011150097474455833\n",
      "Epoch 100 of 100 | MSE: 8.928029160415463e-08\n",
      "Training time: 0.12\n",
      "Validation RMSE: 0.0028764847660661904\n",
      "Test RMSE: 0.003224462631516944\n",
      "Test MEAN: 1.0004501131528534\n",
      "Test VOL: 0.007646408462075337\n",
      "\n",
      "Shallow NNF Training & Results for model 4.0:\n",
      "Epoch 1 of 100 | MSE: 0.06131204217672348\n",
      "Epoch 100 of 100 | MSE: 2.763741235867201e-07\n",
      "Training time: 0.69\n",
      "\n",
      "Deep NNF Training & Results for model 4.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0015420473646372557\n",
      "Epoch 100 of 100 | MSE: 1.5876802095249332e-09\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003490168943618428\n",
      "Test RMSE: 0.003408541190678408\n",
      "Test MEAN: 1.0006669662052787\n",
      "Test VOL: 0.006450177442771172\n",
      "\n",
      "Shallow NNF Training & Results for model 5.0:\n",
      "Epoch 1 of 100 | MSE: 0.04290004074573517\n",
      "Epoch 100 of 100 | MSE: 3.197442310920451e-10\n",
      "Training time: 0.69\n",
      "\n",
      "Deep NNF Training & Results for model 5.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0042230780236423016\n",
      "Epoch 100 of 100 | MSE: 8.679169383185581e-09\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0032507791534562586\n",
      "Test RMSE: 0.0035878355438287756\n",
      "Test MEAN: 0.9998877724816388\n",
      "Test VOL: 0.008120178439764104\n",
      "\n",
      "Shallow NNF Training & Results for model 6.0:\n",
      "Epoch 1 of 100 | MSE: 0.05224202945828438\n",
      "Epoch 100 of 100 | MSE: 9.096225994653651e-10\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 6.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.020443402230739594\n",
      "Epoch 100 of 100 | MSE: 1.1676806366267556e-07\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0024230761395077354\n",
      "Test RMSE: 0.00315338746268622\n",
      "Test MEAN: 1.0000515418323084\n",
      "Test VOL: 0.008456292138542163\n",
      "\n",
      "Shallow NNF Training & Results for model 7.0:\n",
      "Epoch 1 of 100 | MSE: 0.040438927710056305\n",
      "Epoch 100 of 100 | MSE: 3.754863442395617e-08\n",
      "Training time: 0.69\n",
      "\n",
      "Deep NNF Training & Results for model 7.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.004888944327831268\n",
      "Epoch 100 of 100 | MSE: 1.7469780289047776e-08\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0026605678770377094\n",
      "Test RMSE: 0.0035525661109910667\n",
      "Test MEAN: 0.998987611721462\n",
      "Test VOL: 0.011109575808063455\n",
      "\n",
      "Shallow NNF Training & Results for model 8.0:\n",
      "Epoch 1 of 100 | MSE: 0.031945545226335526\n",
      "Epoch 100 of 100 | MSE: 5.1159076974727213e-11\n",
      "Training time: 0.72\n",
      "\n",
      "Deep NNF Training & Results for model 8.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.023081468418240547\n",
      "Epoch 100 of 100 | MSE: 5.565597405166045e-08\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0031695681194232692\n",
      "Test RMSE: 0.0037846058210343606\n",
      "Test MEAN: 0.9995514704777392\n",
      "Test VOL: 0.011730946877937317\n",
      "\n",
      "Shallow NNF Training & Results for model 9.0:\n",
      "Epoch 1 of 100 | MSE: 0.025093920528888702\n",
      "Epoch 100 of 100 | MSE: 7.2852799348765984e-09\n",
      "Training time: 0.71\n",
      "\n",
      "Deep NNF Training & Results for model 9.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.002577043604105711\n",
      "Epoch 100 of 100 | MSE: 3.4262832571130275e-08\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003003499847722314\n",
      "Test RMSE: 0.003829753853813222\n",
      "Test MEAN: 0.9995845552772018\n",
      "Test VOL: 0.011598349316424415\n",
      "\n",
      "Shallow NNF Training & Results for model 10.0:\n",
      "Epoch 1 of 100 | MSE: 0.03367876634001732\n",
      "Epoch 100 of 100 | MSE: 2.8776980798284058e-09\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 10.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0010437171440571547\n",
      "Epoch 100 of 100 | MSE: 2.1683526085780613e-08\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0033068196449536416\n",
      "Test RMSE: 0.004019032748041694\n",
      "Test MEAN: 0.9996615944539536\n",
      "Test VOL: 0.012516163588890966\n",
      "\n",
      "Shallow NNF Training & Results for model 11.0:\n",
      "Epoch 1 of 100 | MSE: 0.037159353494644165\n",
      "Epoch 100 of 100 | MSE: 4.7968562455480424e-08\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 11.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 6.350863259285688e-05\n",
      "Epoch 100 of 100 | MSE: 8.049596544879023e-10\n",
      "Training time: 0.14\n",
      "Validation RMSE: 0.003704026971133029\n",
      "Test RMSE: 0.0034973208708499078\n",
      "Test MEAN: 1.0007159802146965\n",
      "Test VOL: 0.010575951925614065\n",
      "\n",
      "Shallow NNF Training & Results for model 12.0:\n",
      "Epoch 1 of 100 | MSE: 0.041673362255096436\n",
      "Epoch 100 of 100 | MSE: 2.0455104277061764e-08\n",
      "Training time: 0.69\n",
      "\n",
      "Deep NNF Training & Results for model 12.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.00309605966322124\n",
      "Epoch 100 of 100 | MSE: 3.526299252598619e-08\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003659917860858334\n",
      "Test RMSE: 0.0032924723847048963\n",
      "Test MEAN: 0.999931300553951\n",
      "Test VOL: 0.01068186748220453\n",
      "\n",
      "Shallow NNF Training & Results for model 13.0:\n",
      "Epoch 1 of 100 | MSE: 0.03197505325078964\n",
      "Epoch 100 of 100 | MSE: 1.1910689323713086e-08\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 13.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 4.912649455945939e-05\n",
      "Epoch 100 of 100 | MSE: 3.007016857736744e-11\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.0037645820010575363\n",
      "Test RMSE: 0.0033964774022009975\n",
      "Test MEAN: 1.0014745592340046\n",
      "Test VOL: 0.008423752556771089\n",
      "\n",
      "Shallow NNF Training & Results for model 14.0:\n",
      "Epoch 1 of 100 | MSE: 0.031466592103242874\n",
      "Epoch 100 of 100 | MSE: 6.509523586828436e-08\n",
      "Training time: 0.76\n",
      "\n",
      "Deep NNF Training & Results for model 14.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.00035231997026130557\n",
      "Epoch 100 of 100 | MSE: 2.3374102653406226e-10\n",
      "Training time: 0.14\n",
      "Validation RMSE: 0.004210456216539985\n",
      "Test RMSE: 0.0037545901865821867\n",
      "Test MEAN: 1.0007581108568304\n",
      "Test VOL: 0.007679729893244921\n",
      "\n",
      "Shallow NNF Training & Results for model 15.0:\n",
      "Epoch 1 of 100 | MSE: 0.03413666784763336\n",
      "Epoch 100 of 100 | MSE: 9.231882813764969e-09\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 15.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0013417113805189729\n",
      "Epoch 100 of 100 | MSE: 9.094947017729282e-09\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003942107620518554\n",
      "Test RMSE: 0.0033171946753326616\n",
      "Test MEAN: 1.0000268583789516\n",
      "Test VOL: 0.009431439567356672\n",
      "\n",
      "Shallow NNF Training & Results for model 16.0:\n",
      "Epoch 1 of 100 | MSE: 0.04516994580626488\n",
      "Epoch 100 of 100 | MSE: 3.370246304257307e-10\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 16.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.015028981491923332\n",
      "Epoch 100 of 100 | MSE: 1.2125043724608986e-07\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003700460190704058\n",
      "Test RMSE: 0.0031832759095436255\n",
      "Test MEAN: 1.000422589476529\n",
      "Test VOL: 0.008704868629125442\n",
      "\n",
      "Shallow NNF Training & Results for model 17.0:\n",
      "Epoch 1 of 100 | MSE: 0.023990441113710403\n",
      "Epoch 100 of 100 | MSE: 3.2486514101037756e-05\n",
      "Training time: 0.75\n",
      "\n",
      "Deep NNF Training & Results for model 17.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.00011824264947790653\n",
      "Epoch 100 of 100 | MSE: 1.6043486539274454e-09\n",
      "Training time: 0.15\n",
      "Validation RMSE: 0.003605709203284065\n",
      "Test RMSE: 0.003982969981082068\n",
      "Test MEAN: 1.0002617542625383\n",
      "Test VOL: 0.010067214698478055\n",
      "\n",
      "Shallow NNF Training & Results for model 18.0:\n",
      "Epoch 1 of 100 | MSE: 0.023970643058419228\n",
      "Epoch 100 of 100 | MSE: 8.297370368381962e-05\n",
      "Training time: 0.72\n",
      "\n",
      "Deep NNF Training & Results for model 18.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0004821979673579335\n",
      "Epoch 100 of 100 | MSE: 6.46523190539483e-09\n",
      "Training time: 0.15\n",
      "Validation RMSE: 0.0035475791720824767\n",
      "Test RMSE: 0.0036389021364683057\n",
      "Test MEAN: 1.0010925859063964\n",
      "Test VOL: 0.009197982274888682\n",
      "\n",
      "Shallow NNF Training & Results for model 19.0:\n",
      "Epoch 1 of 100 | MSE: 0.019766829907894135\n",
      "Epoch 100 of 100 | MSE: 5.863365348801608e-10\n",
      "Training time: 0.71\n",
      "\n",
      "Deep NNF Training & Results for model 19.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.00016229091852437705\n",
      "Epoch 100 of 100 | MSE: 7.76467778962342e-12\n",
      "Training time: 0.13\n",
      "Validation RMSE: 0.003437234451885243\n",
      "Test RMSE: 0.003682422344138728\n",
      "Test MEAN: 1.0005162702957098\n",
      "Test VOL: 0.0093123737728867\n",
      "\n",
      "Shallow NNF Training & Results for model 20.0:\n",
      "Epoch 1 of 100 | MSE: 0.042254913598299026\n",
      "Epoch 100 of 100 | MSE: 5.577792450139896e-09\n",
      "Training time: 0.71\n",
      "\n",
      "Deep NNF Training & Results for model 20.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.004104222171008587\n",
      "Epoch 100 of 100 | MSE: 7.76609798691652e-09\n",
      "Training time: 0.14\n",
      "Validation RMSE: 0.0035602706118652737\n",
      "Test RMSE: 0.003745086087935177\n",
      "Test MEAN: 1.0007421169091857\n",
      "Test VOL: 0.009125254341887863\n",
      "\n",
      "Shallow NNF Training & Results for model 21.0:\n",
      "Epoch 1 of 100 | MSE: 0.04772099107503891\n",
      "Epoch 100 of 100 | MSE: 1.182801412369372e-09\n",
      "Training time: 0.71\n",
      "\n",
      "Deep NNF Training & Results for model 21.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.0037408953066915274\n",
      "Epoch 100 of 100 | MSE: 1.574216312860699e-08\n",
      "Training time: 0.14\n",
      "Validation RMSE: 0.0035242371074303905\n",
      "Test RMSE: 0.0034063332937688015\n",
      "Test MEAN: 1.0012134705895437\n",
      "Test VOL: 0.006878009908549835\n",
      "\n",
      "Shallow NNF Training & Results for model 22.0:\n",
      "Epoch 1 of 100 | MSE: 0.02655981481075287\n",
      "Epoch 100 of 100 | MSE: 8.656972028120435e-09\n",
      "Training time: 0.72\n",
      "\n",
      "Deep NNF Training & Results for model 22.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 1.7402080629835837e-05\n",
      "Epoch 100 of 100 | MSE: 8.100409232270067e-10\n",
      "Training time: 0.15\n",
      "Validation RMSE: 0.00387207128012887\n",
      "Test RMSE: 0.0030497237281569543\n",
      "Test MEAN: 1.0015905451205058\n",
      "Test VOL: 0.007106866742036096\n",
      "\n",
      "Shallow NNF Training & Results for model 23.0:\n",
      "Epoch 1 of 100 | MSE: 0.026527410373091698\n",
      "Epoch 100 of 100 | MSE: 5.5627854322892745e-08\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 23.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.01098095066845417\n",
      "Epoch 100 of 100 | MSE: 4.859153968084229e-09\n",
      "Training time: 0.14\n",
      "Validation RMSE: 0.003363547883680102\n",
      "Test RMSE: 0.0026277494279935607\n",
      "Test MEAN: 1.000951531033723\n",
      "Test VOL: 0.005190118244226247\n",
      "\n",
      "Shallow NNF Training & Results for model 24.0:\n",
      "Epoch 1 of 100 | MSE: 0.017467401921749115\n",
      "Epoch 100 of 100 | MSE: 5.835974903334318e-08\n",
      "Training time: 0.70\n",
      "\n",
      "Deep NNF Training & Results for model 24.0 (Partial replication):\n",
      "Epoch 1 of 100 | MSE: 0.006578629370778799\n",
      "Epoch 100 of 100 | MSE: 7.1034065740605e-08\n",
      "Training time: 0.15\n",
      "Validation RMSE: 0.0036692486834182837\n",
      "Test RMSE: 0.002834379759141487\n",
      "Test MEAN: 1.0015094926803192\n",
      "Test VOL: 0.0057389862969937505\n",
      "Selected Model Test Results for model i = 1 are: \n",
      "RMSE = 0.002720049127052518\n",
      "MEAN = 0.9999396353758883\n",
      "VOL = 0.009935167920603755\n"
     ]
    }
   ],
   "source": [
    "#shallow nnf\n",
    "'''\n",
    "in this cell,firstly, train, validation and test datasets are sliced in each loop. then shallow NNf outputs the best stocks with \n",
    "full replication and then we use the specific stocks to train the model again and get the optimal weights (each loop)\n",
    "then best model will be chosen. Also RMSE, Mean and volatility of all models and then the best model is printed.\n",
    "'''\n",
    "shallow_nnf_valid_rmse_list = []\n",
    "shallow_nnf_test_results = []\n",
    "shallow_nnf_test_plot = [] # storing the shallow model test data return for plotting later on\n",
    "\n",
    "for i in range(int(24/rbp)):\n",
    "    df_reduce = df.copy()\n",
    "    x_train = data_process(date_slicer(df, '2014-07-01', 36, i*rbp))\n",
    "    y_train = data_process(date_slicer(df_sp, '2014-07-01', 36, i*rbp))\n",
    "    x_valid = data_process(date_slicer(df, '2017-07-01', 6, i*rbp))\n",
    "    y_valid = data_process(date_slicer(df_sp, '2017-07-01', 6, i*rbp))\n",
    "    x_test = data_process(date_slicer(df, '2018-01-01', 1, i*rbp))\n",
    "    y_test = data_process(date_slicer(df_sp, '2018-01-01', 1, i*rbp))\n",
    "    weights = train_shallow_nnf(x_train, y_train, i*rbp)\n",
    "    x_train, x_valid, x_test, out_index = partial(x_train, x_valid, x_test, weights, stocks_index, num = partial_num)\n",
    "    df_reduce = df_reduce.drop(out_index, axis=1)\n",
    "    train_shallow_nnf_partial(x_train, y_train, i*rbp)\n",
    "    shallow_nnf_valid_rmse_list.append(valid_fun(x_valid, i*rbp, shallow_NNF_partial))\n",
    "    shallow_nnf_test_results.append(test_fun(x_test, i*rbp, shallow_NNF_partial))\n",
    "    portfolio_return(df_reduce, x_test, shallow_NNF_partial, i, shallow_nnf_test_plot)\n",
    "    shallow_NNF.reset_parameters()\n",
    "    shallow_NNF_partial.reset_parameters()\n",
    "\n",
    "# print(f'\\nMin Valid RMSE is: {min(valid_rmse_list)} for model i = {(deep_best_result_index)+1}')\n",
    "print('Selected Model Test Results for model i =', (deep_best_result_index)+1, 'are: ')\n",
    "print('RMSE =', shallow_nnf_test_results[(deep_best_result_index)]['RMSE'])\n",
    "print('MEAN =', shallow_nnf_test_results[(deep_best_result_index)]['MEAN'])\n",
    "print('VOL =', shallow_nnf_test_results[(deep_best_result_index)]['VOL'])\n",
    "\n",
    "shallow_nnf_test_plot = np.array(shallow_nnf_test_plot).reshape(-1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1/N Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Equal Weights Model Results for model 1:\n",
      "Validation RMSE: 0.0020402585058858147\n",
      "Test RMSE: 0.0024690427967778684\n",
      "Test MEAN: 1.000160710217343\n",
      "Test VOL: 0.009608459166304966\n",
      "\n",
      "Equal Weights Model Results for model 2:\n",
      "Validation RMSE: 0.002545838766648022\n",
      "Test RMSE: 0.0025876256200170708\n",
      "Test MEAN: 1.0003608661568495\n",
      "Test VOL: 0.009700560585897602\n",
      "\n",
      "Equal Weights Model Results for model 3:\n",
      "Validation RMSE: 0.0029352284307108257\n",
      "Test RMSE: 0.0028629910274078933\n",
      "Test MEAN: 1.0007484612334512\n",
      "Test VOL: 0.007222356462512702\n",
      "\n",
      "Equal Weights Model Results for model 4:\n",
      "Validation RMSE: 0.002457553805160017\n",
      "Test RMSE: 0.0021819019247381055\n",
      "Test MEAN: 1.0010341479600036\n",
      "Test VOL: 0.005987441057910082\n",
      "\n",
      "Equal Weights Model Results for model 5:\n",
      "Validation RMSE: 0.0028512759548855685\n",
      "Test RMSE: 0.0031224419238510587\n",
      "Test MEAN: 1.000226484557208\n",
      "Test VOL: 0.006970729181933849\n",
      "\n",
      "Equal Weights Model Results for model 6:\n",
      "Validation RMSE: 0.0024895274568700514\n",
      "Test RMSE: 0.0029563535527749674\n",
      "Test MEAN: 1.000446868177007\n",
      "Test VOL: 0.008283085357769662\n",
      "\n",
      "Equal Weights Model Results for model 7:\n",
      "Validation RMSE: 0.0025607255773944545\n",
      "Test RMSE: 0.0028581441112021035\n",
      "Test MEAN: 0.9997016211003428\n",
      "Test VOL: 0.009918531854351008\n",
      "\n",
      "Equal Weights Model Results for model 8:\n",
      "Validation RMSE: 0.0033126572949459293\n",
      "Test RMSE: 0.004439246378213979\n",
      "Test MEAN: 1.0001068620517368\n",
      "Test VOL: 0.009514536744932928\n",
      "\n",
      "Equal Weights Model Results for model 9:\n",
      "Validation RMSE: 0.002283009811689176\n",
      "Test RMSE: 0.0028874572698182974\n",
      "Test MEAN: 0.9999381220152354\n",
      "Test VOL: 0.011998963229311354\n",
      "\n",
      "Equal Weights Model Results for model 10:\n",
      "Validation RMSE: 0.002648502952803946\n",
      "Test RMSE: 0.0034259179108101123\n",
      "Test MEAN: 0.9999930998367046\n",
      "Test VOL: 0.011417139475192472\n",
      "\n",
      "Equal Weights Model Results for model 11:\n",
      "Validation RMSE: 0.002599489104377707\n",
      "Test RMSE: 0.0026363443518920503\n",
      "Test MEAN: 1.0008433039750038\n",
      "Test VOL: 0.010549274990998583\n",
      "\n",
      "Equal Weights Model Results for model 12:\n",
      "Validation RMSE: 0.0032960302558993986\n",
      "Test RMSE: 0.002662334653095103\n",
      "Test MEAN: 0.9999339312390889\n",
      "Test VOL: 0.010088519716182054\n",
      "\n",
      "Equal Weights Model Results for model 13:\n",
      "Validation RMSE: 0.003031265546761551\n",
      "Test RMSE: 0.002542829932807525\n",
      "Test MEAN: 1.0014953624152791\n",
      "Test VOL: 0.008052065245749333\n",
      "\n",
      "Equal Weights Model Results for model 14:\n",
      "Validation RMSE: 0.00343574504196367\n",
      "Test RMSE: 0.0026340477328911506\n",
      "Test MEAN: 1.000964110603057\n",
      "Test VOL: 0.006558927377785123\n",
      "\n",
      "Equal Weights Model Results for model 15:\n",
      "Validation RMSE: 0.003404539902394645\n",
      "Test RMSE: 0.0027583114729650172\n",
      "Test MEAN: 1.000405648527037\n",
      "Test VOL: 0.009126468942365218\n",
      "\n",
      "Equal Weights Model Results for model 16:\n",
      "Validation RMSE: 0.0030231434642299633\n",
      "Test RMSE: 0.002413542174174387\n",
      "Test MEAN: 1.0006822527790271\n",
      "Test VOL: 0.007632043181226953\n",
      "\n",
      "Equal Weights Model Results for model 17:\n",
      "Validation RMSE: 0.002923303000527995\n",
      "Test RMSE: 0.002846690844966686\n",
      "Test MEAN: 1.000542163106458\n",
      "Test VOL: 0.00900368818495326\n",
      "\n",
      "Equal Weights Model Results for model 18:\n",
      "Validation RMSE: 0.0029858226498528247\n",
      "Test RMSE: 0.002345976069619469\n",
      "Test MEAN: 1.0011339622550546\n",
      "Test VOL: 0.008027371394341959\n",
      "\n",
      "Equal Weights Model Results for model 19:\n",
      "Validation RMSE: 0.0030997215925497885\n",
      "Test RMSE: 0.0022810350354254957\n",
      "Test MEAN: 1.000620534080882\n",
      "Test VOL: 0.008129976699153996\n",
      "\n",
      "Equal Weights Model Results for model 20:\n",
      "Validation RMSE: 0.002847978959830675\n",
      "Test RMSE: 0.002345432725186153\n",
      "Test MEAN: 1.0007962256182252\n",
      "Test VOL: 0.007750981698728687\n",
      "\n",
      "Equal Weights Model Results for model 21:\n",
      "Validation RMSE: 0.0025246997702295863\n",
      "Test RMSE: 0.0023027967070906536\n",
      "Test MEAN: 1.001326288754481\n",
      "Test VOL: 0.006377969142692981\n",
      "\n",
      "Equal Weights Model Results for model 22:\n",
      "Validation RMSE: 0.0022233860600057104\n",
      "Test RMSE: 0.001959799258483173\n",
      "Test MEAN: 1.0018925277505208\n",
      "Test VOL: 0.006276066879456749\n",
      "\n",
      "Equal Weights Model Results for model 23:\n",
      "Validation RMSE: 0.0029806445854107647\n",
      "Test RMSE: 0.002289446400829517\n",
      "Test MEAN: 1.001178119886419\n",
      "Test VOL: 0.004738643814543081\n",
      "\n",
      "Equal Weights Model Results for model 24:\n",
      "Validation RMSE: 0.0027606434631875297\n",
      "Test RMSE: 0.0016757020028151796\n",
      "Test MEAN: 1.0017689025935597\n",
      "Test VOL: 0.004743674177191311\n",
      "Selected Model Test Results for model i = 1 are: \n",
      "RMSE = 0.0024690427967778684\n",
      "MEAN = 1.000160710217343\n",
      "VOL = 0.009608459166304966\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "here we run the 1/N model, for the number of stocks, each stock gets the weight of 1/N meaning that\n",
    "every stock is equally important, this model play the role of a benchmark to see how effective our model are\n",
    "'''\n",
    "equal_w_model_valid_rmse_list = []\n",
    "equal_w_model_test_results = []\n",
    "equal_w_model_test_plot = [] # storing the 1/n model test data return for plotting later on\n",
    "\n",
    "for i in range(int(24/rbp)):\n",
    "    df_reduce = df.copy()\n",
    "    df_reduce = df_reduce.drop(out_index_history[i], axis=1)\n",
    "    print(f'\\nEqual Weights Model Results for model {i+1}:')\n",
    "    x_train = data_process(date_slicer(df_reduce, '2014-07-01', 36, i*rbp))\n",
    "    y_train = data_process(date_slicer(df_sp, '2014-07-01', 36, i*rbp))\n",
    "    x_valid = data_process(date_slicer(df_reduce, '2017-07-01', 6, i*rbp))\n",
    "    y_valid = data_process(date_slicer(df_sp, '2017-07-01', 6, i*rbp))\n",
    "    x_test = data_process(date_slicer(df_reduce, '2018-01-01', 1, i*rbp))\n",
    "    y_test = data_process(date_slicer(df_sp, '2018-01-01', 1, i*rbp))\n",
    "    \n",
    "    equal_w_model_valid_rmse_list.append(valid_fun(x_valid, i*rbp, equal_w_model))\n",
    "    equal_w_model_test_results.append(test_fun(x_test, i*rbp, equal_w_model))\n",
    "    portfolio_return(df_reduce, x_test, equal_w_model, i, equal_w_model_test_plot)\n",
    "    \n",
    "print('Selected Model Test Results for model i =', (deep_best_result_index)+1, 'are: ')\n",
    "print('RMSE =', equal_w_model_test_results[(deep_best_result_index)]['RMSE'])\n",
    "print('MEAN =', equal_w_model_test_results[(deep_best_result_index)]['MEAN'])\n",
    "print('VOL =', equal_w_model_test_results[(deep_best_result_index)]['VOL'])\n",
    "\n",
    "equal_w_model_test_plot = np.array(equal_w_model_test_plot).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models test results with rebalancing period of 1 month(s) are: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Deep NNF</th>\n",
       "      <th>Shallow NNF</th>\n",
       "      <th>1/N Model</th>\n",
       "      <th>S&amp;P 500</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RMSE</th>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEAN</th>\n",
       "      <td>1.000157</td>\n",
       "      <td>0.999940</td>\n",
       "      <td>1.000161</td>\n",
       "      <td>1.000121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VOL</th>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.010367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Deep NNF  Shallow NNF  1/N Model   S&P 500\n",
       "RMSE  0.002463     0.002720   0.002469         -\n",
       "MEAN  1.000157     0.999940   1.000161  1.000121\n",
       "VOL   0.009617     0.009935   0.009608  0.010367"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print test results\n",
    "'''\n",
    "here we compare the results in a dataframe featuring RMSE, MEAN and, volatility of each model in the test dataset\n",
    "that has the best results for deep nnf model. this dataframe can cope with the understanding of why we bother \n",
    "implementing a complex neural network\n",
    "'''\n",
    "print(f'Models test results with rebalancing period of {rbp} month(s) are: ')\n",
    "deep_temp = pd.DataFrame(deep_nnf_test_results)\n",
    "deep_temp = deep_temp.iloc[deep_best_result_index]\n",
    "shallow_temp = pd.DataFrame(shallow_nnf_test_results)\n",
    "shallow_temp = shallow_temp.iloc[deep_best_result_index]\n",
    "equal_w_temp = pd.DataFrame(equal_w_model_test_results)\n",
    "equal_w_temp = equal_w_temp.iloc[deep_best_result_index]\n",
    "\n",
    "# extract the mean and volatility of the s&p index on the test dataset\n",
    "sp_temp_rmse = '-'\n",
    "sp_temp_mean = daily_return(date_slicer(df_sp, '2018-01-01', 6, deep_best_result_index)).mean()[0]\n",
    "sp_temp_std = daily_return(date_slicer(df_sp, '2018-01-01', 6, deep_best_result_index)).std()[0]\n",
    "sp_temp = pd.DataFrame([sp_temp_rmse, sp_temp_mean, sp_temp_std], index=deep_temp.index)\n",
    "\n",
    "# concatinating the result in a unified dataframe\n",
    "final_result = pd.concat([deep_temp, shallow_temp, equal_w_temp, sp_temp], axis=1, join='inner')\n",
    "final_result.columns = ['Deep NNF', 'Shallow NNF', '1/N Model', 'S&P 500']\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of test RMSE for each model: \n",
      "Deep NNF: 0.0026441713799887443\n",
      "Shallow NNF: 0.0034002470515819235\n",
      "Equal weight model: 0.002645225494910542\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to further showcase the results, here we compute the average RMSE of each model in test dataset\n",
    "'''\n",
    "print(f'Average of test RMSE for each model: ')\n",
    "\n",
    "deep_nnf_test_rmse_mean = 0 # temp variable for storing each tmse for deep nnf model\n",
    "for i in range(int(24/rbp)):\n",
    "    deep_nnf_test_rmse_mean += deep_nnf_test_results[i]['RMSE']\n",
    "print(f'Deep NNF: {deep_nnf_test_rmse_mean/int(24/rbp)}')\n",
    "\n",
    "shallow_nnf_test_rmse_mean = 0 # temp variable for storing each tmse for shallow nnf model\n",
    "for i in range(int(24/rbp)):\n",
    "    shallow_nnf_test_rmse_mean += shallow_nnf_test_results[i]['RMSE']\n",
    "print(f'Shallow NNF: {shallow_nnf_test_rmse_mean/int(24/rbp)}')\n",
    "\n",
    "equal_w_model_test_rmse_mean = 0 # temp variable for storing each tmse for 1/n model model\n",
    "for i in range(int(24/rbp)):\n",
    "    equal_w_model_test_rmse_mean += equal_w_model_test_results[i]['RMSE']\n",
    "print(f'Equal weight model: {equal_w_model_test_rmse_mean/int(24/rbp)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinating the test dataset return results of each model + index return for plot\n",
    "plot_test = pd.concat([pd.DataFrame(equal_w_model_test_plot), pd.DataFrame(shallow_nnf_test_plot),\n",
    "                       pd.DataFrame(deep_nnf_test_plot), pd.DataFrame(index_test_plot)], axis=1, join='inner')\n",
    "\n",
    "plot_test.columns = ['1/N Model', 'Shallow NNF', 'Deep NNF', 'S&P 500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "line": {
          "color": "rgba(255, 153, 51, 1.0)",
          "dash": "solid",
          "shape": "linear",
          "width": 1.3
         },
         "mode": "lines",
         "name": "1/N Model",
         "text": "",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477
         ],
         "y": [
          1.0062676172526308,
          1.0081373676787257,
          1.0125131757629535,
          1.0131336025245914,
          1.0140962084113216,
          1.0092517024882537,
          1.017563373772811,
          1.0215155331543533,
          1.0155329744159463,
          1.022516909030883,
          1.022840681941257,
          1.029008509944538,
          1.0355318001044767,
          1.0377491798568295,
          1.0351488228943735,
          1.0368083968648878,
          1.0486690287323688,
          1.0421276975136546,
          1.0322581889519156,
          1.0307325062616097,
          0.978619685140137,
          0.9437409528062374,
          0.9553963794136182,
          0.950189399620171,
          0.9190094927117505,
          0.9320861637458316,
          0.9429556017907292,
          0.9458680755631698,
          0.9633203203322742,
          0.9728221736347036,
          0.9727024929545001,
          0.9669755507164752,
          0.963595341484127,
          0.9641432064223538,
          0.9796780397174724,
          0.9851835310350824,
          0.9729991737858936,
          0.9628906270422349,
          1.0058561493549647,
          1.0184317397609917,
          1.0234336834131754,
          1.0222334482799627,
          1.0268137235093575,
          1.0436970985510776,
          1.0407108524698367,
          1.0386850008934554,
          1.0347590778422104,
          1.0333204589726352,
          1.036653359449439,
          1.0281726949630379,
          1.030121724359793,
          1.0277271410963416,
          1.0058716124386446,
          0.9867416410238288,
          1.0095592204511217,
          0.9985920502285717,
          1.0009989702653261,
          1.0147689060356688,
          1.0141571500864428,
          1.026131685788871,
          1.032779724987422,
          1.0112011763090383,
          1.0122133854559046,
          1.029074374976798,
          1.0232267350920046,
          1.0292564792870862,
          1.0265231598857485,
          1.0352759950854056,
          1.0434903204486055,
          1.0456816564754212,
          1.0383068013932124,
          1.02751639789064,
          1.0287474098500906,
          1.0187573935751224,
          1.0212308807076882,
          1.0352590070883327,
          1.0389497326218964,
          1.029954704023546,
          0.9942960002645669,
          0.9898657629060906,
          1.0038104616503014,
          1.0054347795959102,
          1.0057615446546968,
          1.014785858409811,
          1.0238434412655386,
          1.0204267251734165,
          1.022948495579923,
          1.020438210776554,
          1.0256300029262129,
          1.0263616625928753,
          1.021210639286996,
          1.0284738729726632,
          1.0277868282158122,
          1.0273635993908525,
          1.0288381675414326,
          1.0298292320179487,
          1.0196770350745978,
          1.0295581633880195,
          1.0233082133531124,
          1.007966309140573,
          1.0095293918705008,
          1.0180393627359963,
          1.0182322438076858,
          1.025050768370751,
          1.0264984576393836,
          1.0299961895773504,
          1.0267544470127274,
          1.029844544162624,
          1.0308687383169326,
          1.0285242122089124,
          1.022093328912376,
          1.0251267079481519,
          1.0198579269434154,
          1.0227343296691052,
          1.014021791731769,
          1.013904967575662,
          1.0040517266112523,
          1.0118090476764858,
          1.0120817301772356,
          0.9962539892083371,
          1.0049408880972932,
          1.011687059475301,
          1.0220539008414642,
          1.0276651328464617,
          1.027954508891701,
          1.036463425687162,
          1.035441940257798,
          1.0356371252944525,
          1.0383587993359555,
          1.0395089062115708,
          1.0368624729161067,
          1.033457330024739,
          1.0346154098953289,
          1.0320465842272781,
          1.0425013350314924,
          1.0507075163748465,
          1.043657075631381,
          1.0367006157835106,
          1.0445596046811292,
          1.005141418981641,
          1.009076702450313,
          1.0098353511280198,
          1.0103496359625315,
          1.0104671286865077,
          1.011143804109723,
          0.9994437075470038,
          0.99565077183963,
          1.0036753223436372,
          1.0004977045308485,
          1.0086752548513287,
          1.0132425070919622,
          1.0156813113843062,
          1.017369625403544,
          1.0130682276849783,
          1.0103718914289335,
          1.0155414675533065,
          1.018651812358888,
          1.0186576456720056,
          1.0205113259977034,
          1.0150182196762532,
          1.0161885928490266,
          0.9990946895387253,
          0.9935830167713895,
          0.9896530236635233,
          0.9924345957378736,
          0.9925007260119059,
          0.9963079283315062,
          1.0002705672636791,
          1.0038353827272997,
          0.9995099418515249,
          1.0024199960020734,
          1.001544050729199,
          1.009050947033026,
          1.0114750191953645,
          1.006684313802626,
          1.004999435910754,
          0.9997550009809274,
          1.0032522490640254,
          1.0065134634831152,
          1.0007251579756498,
          1.0001884692615475,
          0.9940952457839094,
          0.9899338810261029,
          0.9902975176421791,
          0.9848562268658618,
          0.9567830249832061,
          0.9372297490763087,
          0.9435322458496118,
          0.943474269525047,
          0.9627190338342412,
          0.9611365287509082,
          0.9472195484676298,
          0.9458759354689725,
          0.9373696014297732,
          0.929321845291068,
          0.9006604689923742,
          0.9098925429488184,
          0.9003899238095467,
          0.9011832454474492,
          0.9194286663631426,
          0.9265486355274616,
          0.9971331933732974,
          1.0053941346904378,
          1.0124461393985802,
          1.0297915717056192,
          1.0220987611338177,
          1.0123949292191208,
          0.9980469028176246,
          0.9965810882228967,
          0.9933996582044836,
          1.0055508778693887,
          1.0101530317618495,
          0.9962709566260985,
          0.9802850858452985,
          0.9864833062200646,
          0.9810565989152972,
          0.9969474845407783,
          0.9956697248473115,
          1.014360074019292,
          1.012348202138001,
          1.0152853247552729,
          0.9658764291297905,
          0.9633113382011266,
          0.9440845169093361,
          0.9400731487719868,
          0.9366118158776926,
          0.9404229394899458,
          0.9356217218707389,
          0.9244951149643832,
          0.9039643651928988,
          0.9036705866450048,
          0.8874237031878657,
          0.872719266101339,
          0.857786986809128,
          0.8336787245797506,
          0.8687199131454718,
          0.8757482611409707,
          0.8754846192049873,
          0.8826026088439742,
          0.9811368762157169,
          1.0130389739521002,
          1.0275593356561288,
          1.0382738855649845,
          1.0440601833788623,
          1.049607418370916,
          1.0490909102581443,
          1.0385893876777463,
          1.0491449415342065,
          1.051537772847914,
          1.0621806235070839,
          1.0747218119690063,
          1.0595730433431079,
          1.0586762762009416,
          1.0686741090167586,
          1.0818147209894398,
          1.0777280296818272,
          1.081260409467504,
          1.0916081828954578,
          1.1074605177009977,
          1.0059244296140633,
          1.0106938122641282,
          1.0095333383816683,
          1.00696587973514,
          1.0069594542920088,
          1.0090972375944347,
          1.0189970875823562,
          1.0213393282819567,
          1.0198484045010903,
          1.0315401043995525,
          1.035043408098839,
          1.037273512775653,
          1.0317890940677794,
          1.0368888202749125,
          1.0374323451150729,
          1.0365868802633365,
          1.035089382745919,
          1.035374788007251,
          0.9968532345308485,
          0.993843837086334,
          0.9864356423450796,
          0.977678470104707,
          0.97600135487123,
          0.9879378394209413,
          0.9902514519701412,
          0.9948091381841477,
          0.9934528729188169,
          0.9971179182108871,
          1.0008500679179202,
          0.9978314641753808,
          0.9899179572358354,
          1.0022919886607855,
          0.9809984002583059,
          0.9813652760734167,
          0.9889604961177495,
          0.9838615277491429,
          0.9886530201638724,
          0.99537336193313,
          0.9989076780343974,
          1.0005716181260953,
          1.0027444377816077,
          1.0078888810082631,
          1.008888828988551,
          1.0009429941974748,
          1.006287757583334,
          1.0083735174744275,
          1.0143492054606784,
          1.0111914198954195,
          1.0113001195113003,
          1.0056939595033076,
          1.0049775964704293,
          1.0042849245617675,
          1.0169331346444892,
          1.0135943639528735,
          1.010069457230639,
          1.017263246156714,
          1.0175140565982,
          1.0238510242007028,
          1.000555275797696,
          1.01052842745075,
          1.0037316314443891,
          0.9892896852901018,
          0.9859766860597234,
          0.9871357186960947,
          0.991104410350258,
          0.9663733845093865,
          0.9743453140987016,
          0.9774782485136388,
          0.9855698358243612,
          0.9787028268272407,
          0.9743806111148651,
          0.9838457878230421,
          0.9772542122456191,
          0.9648302645980813,
          0.9650325464725124,
          0.953442733395369,
          0.945119522094652,
          0.9475208702508952,
          0.9390994015721559,
          1.022319882695113,
          1.0331545366112003,
          1.0370170137719463,
          1.0438001960661634,
          1.046792560930738,
          1.0450230968332916,
          1.0454868418490402,
          1.0492582563316795,
          1.0446737240808661,
          1.0455250702436847,
          1.0559653566851182,
          1.0596248476441676,
          1.0679251265719294,
          1.0649899645871956,
          1.0613133774881032,
          1.0532627710566616,
          1.0493568464340313,
          1.05247691726782,
          1.0636914431580637,
          1.000145405648481,
          1.009011334602456,
          1.0083849576089043,
          1.002023990064262,
          1.0025298619004357,
          1.002117402848222,
          1.0039034018125017,
          1.010840338908744,
          1.008572313962448,
          1.006717424657892,
          1.0014526519746791,
          1.0039751938039885,
          1.0000400906201974,
          1.0001137599711925,
          1.006943523740351,
          1.0137410933275368,
          1.0106979897593869,
          1.0158835492939478,
          1.0134320994666441,
          1.0110099869884708,
          1.0009000940204782,
          0.9963292848059946,
          0.9690685086718385,
          0.9817436934350928,
          0.9853433216909881,
          1.0017585885630758,
          0.9985168770684714,
          0.9903794809196037,
          1.0005181718212712,
          0.9721675295046693,
          0.9787988531520772,
          0.9911651847746658,
          1.0036836864776848,
          0.9949746051127623,
          1.0065877270589998,
          1.0077728349798338,
          0.9842156105638165,
          0.9939123276752477,
          0.9909719413999722,
          0.9971087500836917,
          1.009042283065375,
          1.009936502590018,
          1.0133287358757619,
          1.0280288016659285,
          1.0305455736840146,
          1.0336467929742261,
          1.0380895120680766,
          1.0419590366074942,
          1.0469649889675132,
          1.0478077857168906,
          1.0523226714094311,
          1.0521253995719648,
          1.0495621375885176,
          1.045606385105311,
          1.0397657863577192,
          1.0421054879456624,
          1.0343617767591833,
          1.0374282352061373,
          1.03589421502193,
          1.030526771734028,
          1.035100566041648,
          0.9815225204287752,
          0.9890941514117803,
          1.0015760222374903,
          0.997249584006487,
          0.9789639222326668,
          0.9862632333674558,
          0.9938979177940701,
          1.0101334133197464,
          1.0085534780165968,
          1.0172702926878983,
          1.0132389860049442,
          1.0177580742921404,
          1.0136030849445399,
          1.0232504735131966,
          1.0237031123228892,
          1.0251126132074186,
          1.0257007891919556,
          1.0303170167735902,
          1.034767867042786,
          1.035248535862385,
          1.040836178393598,
          1.0356722637784033,
          1.0076589773936924,
          1.0103524585683437,
          1.0094050914510302,
          1.0109835986940714,
          1.008241812574126,
          1.008504145984684,
          1.0103731408801766,
          1.0078978735398283,
          1.0105028334735733,
          1.0156419286655864,
          1.0150199137591145,
          1.0137234842694967,
          1.010589867447848,
          1.0066599248707033,
          1.0097055990155437,
          1.0169900855619807,
          1.0199149575539452,
          1.0246497637103096,
          1.018798935545428,
          0.9924317986670339,
          1.0003263822693502,
          1.0029581060714827,
          1.012401543130443,
          1.0090113905041393,
          1.007324862417747,
          1.0118676484032356,
          1.0230711671463713,
          1.0203533173625667,
          1.0283598452002778,
          1.0293095669439074,
          1.0309142246132466,
          1.0326325383702042,
          1.0361774607696423,
          1.0371417781028276,
          1.0385741874477286,
          1.0408030030668383,
          1.0387410442370348,
          1.0341601551671005
         ]
        },
        {
         "line": {
          "color": "rgba(55, 128, 191, 1.0)",
          "dash": "solid",
          "shape": "linear",
          "width": 1.3
         },
         "mode": "lines",
         "name": "Shallow NNF",
         "text": "",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477
         ],
         "y": [
          1.0069168371659327,
          1.0107877253501658,
          1.0165329356155375,
          1.0183455311718637,
          1.0180295594827031,
          1.0152280800195925,
          1.0278416087379323,
          1.0324325204989835,
          1.0240530249492332,
          1.0341570402497788,
          1.032651235252834,
          1.0418657001082332,
          1.0454584149203885,
          1.0441199288837293,
          1.0452178467165403,
          1.0451194771311692,
          1.0545796218715104,
          1.0449400265030868,
          1.0300290650784671,
          1.027886672909397,
          0.9811419076458749,
          0.944354690145757,
          0.9570079052121487,
          0.958733320115237,
          0.9195170641893557,
          0.9314111101095875,
          0.9425661313694746,
          0.9451629903765766,
          0.962752073099184,
          0.9729429601769003,
          0.9721457276982586,
          0.966885570636071,
          0.96175008853746,
          0.9581807179436092,
          0.9727129275340676,
          0.9772682585779106,
          0.966483938568018,
          0.957012466570945,
          1.0107754587216884,
          1.021090976156776,
          1.0247500552558866,
          1.0221879164001875,
          1.0241956250929665,
          1.0401193972560936,
          1.0383236219427787,
          1.0346633940539234,
          1.0274771067248853,
          1.0237125969286653,
          1.027619385364597,
          1.0171907450948088,
          1.0137805149650678,
          1.0130735726750308,
          0.988596239504198,
          0.9688764127782801,
          0.9914461028710935,
          0.9823063067870893,
          0.9821949561286345,
          0.9951486330363923,
          1.0125215494116369,
          1.0300068005745007,
          1.0384177953419151,
          1.0154112123636996,
          1.0151864681721228,
          1.028597303004924,
          1.0229672168748307,
          1.026245266579425,
          1.0197233783138093,
          1.0283186342729846,
          1.0338395832115435,
          1.034690781117092,
          1.0292837536125743,
          1.0234206489058966,
          1.0282742908606344,
          1.0186658041875176,
          1.0215497783477883,
          1.0304274578489268,
          1.03870961380563,
          1.0264752670179056,
          0.9891075132307026,
          0.9792968990031755,
          0.991671575084308,
          0.9915640926210123,
          0.9881937696104643,
          0.992908944147673,
          1.0018540388931634,
          1.005312438740033,
          1.0054077627519746,
          1.0020715050753097,
          1.0114815434093423,
          1.0149423715488652,
          1.0109118511073867,
          1.0155239808033645,
          1.0103782638655592,
          1.0108716573756071,
          1.008585864301332,
          1.0086071024108676,
          0.997327659158928,
          1.0108148004807833,
          0.9998142958905771,
          1.0073188467661385,
          1.0089731737583147,
          1.0180611035968032,
          1.0170228846968739,
          1.0242999451927985,
          1.0273638250618864,
          1.0302295226052414,
          1.024406498835294,
          1.027881261000316,
          1.0310545625378889,
          1.028110896095943,
          1.0276502292361798,
          1.0303297313417832,
          1.0227881712649365,
          1.021939136169062,
          1.0104164952453212,
          1.0089488338382777,
          0.9988789542057858,
          1.0021239528057926,
          1.0029619686056772,
          1.0001287238890688,
          1.0078488524570224,
          1.0158463481140878,
          1.0270934034592119,
          1.031122380756486,
          1.02265547968936,
          1.0271589691644705,
          1.0293836554487832,
          1.0277409119309902,
          1.0337019312652658,
          1.0361037954347063,
          1.0336576793545071,
          1.0302653950504126,
          1.031088746067788,
          1.027088964852801,
          1.0306980874983591,
          1.0374257058705385,
          1.030998213480477,
          1.030585808759985,
          1.0397198749358403,
          1.005425838754541,
          1.011014907651121,
          1.0136009073516994,
          1.0130472448159147,
          1.0130557509166,
          1.013753572014087,
          1.0052107169532007,
          1.0001979040889233,
          1.0087564141630643,
          0.9949867788074975,
          1.005844318556182,
          1.0130973630036164,
          1.020041187243813,
          1.0271119722519952,
          1.021066481807283,
          1.0162813331919474,
          1.0196724460199271,
          1.0262709672490344,
          1.023697548812268,
          1.0262970339999302,
          1.0197146102139463,
          1.0208645453889729,
          1.0004979197170747,
          0.9971441309835651,
          0.9949792531865499,
          0.9983276601831503,
          1.0013546641669517,
          1.0037934148108851,
          1.0071775930152147,
          1.002898730629677,
          1.000805777712539,
          1.0051746178402308,
          1.0037229595300912,
          1.0097105490163651,
          1.0103595528446019,
          1.0012395822669236,
          0.9944953088845104,
          0.9911009090505815,
          0.9902513751703731,
          0.988425241259303,
          0.9986656553695232,
          1.0013041861490166,
          0.9939260850935121,
          0.9885040476664348,
          0.9895816946943454,
          0.9820156064320315,
          0.9562267994383857,
          0.9319170783747353,
          0.938559030432152,
          0.9402136930922951,
          0.9577066848284614,
          0.9537994701407709,
          0.9401401634362694,
          0.9373404483201486,
          0.9294503491470876,
          0.9257622813296706,
          0.8905378785663388,
          0.9028727087073569,
          0.8922592459708599,
          0.8891357294414756,
          0.9096894390880926,
          0.912876757973452,
          0.9942313544365353,
          1.0049911913149878,
          1.0095269484971436,
          1.0210524043974467,
          1.0121185471662286,
          1.005561024523369,
          0.9934674210461937,
          0.9914435585494332,
          0.9839967739082662,
          0.9929479977356533,
          0.9958642348646242,
          0.9859792797561567,
          0.9674226635390779,
          0.9767069685497112,
          0.969767466282881,
          0.9832454187623141,
          0.9819766250500234,
          1.0001727895401875,
          0.9934905012984074,
          1.0020999731877869,
          0.9667564796818849,
          0.9635213896060955,
          0.9422461022433173,
          0.9379066528942326,
          0.9345157576451625,
          0.9426187138011975,
          0.9358164093359088,
          0.9222134041795107,
          0.9059610386173792,
          0.9045963169417646,
          0.8919765153846672,
          0.8764846376945427,
          0.8552815012139589,
          0.8339853425058167,
          0.8737791497624572,
          0.881910730245253,
          0.8781881612710182,
          0.8840353902127064,
          0.9840869285163731,
          1.0201408390197506,
          1.030731626263048,
          1.0429935752844128,
          1.0501529491207482,
          1.0506793333116156,
          1.052336126948381,
          1.0509521151621972,
          1.0571551322496875,
          1.0589135888222327,
          1.0688840875274657,
          1.0849598477069637,
          1.0685085794127878,
          1.06947814010204,
          1.0763750831808174,
          1.0881516858078062,
          1.0845841464320762,
          1.0867570430741365,
          1.0899442007639957,
          1.0981260416485894,
          1.0050133106863002,
          1.0103100050947285,
          1.0059545417151363,
          1.0054685979171447,
          1.007789086346224,
          1.011602127555183,
          1.0277024478820098,
          1.0341786296746913,
          1.0250948483650961,
          1.039203429461427,
          1.040482081121944,
          1.043342222711725,
          1.0392956069623212,
          1.0391865318623061,
          1.0398583958977599,
          1.0359009326503006,
          1.0371216277882953,
          1.0365015138502605,
          0.9963508149137158,
          0.9924485103723099,
          0.9823749493641015,
          0.9746984142501012,
          0.9725525439116689,
          0.9846026940356761,
          0.9871309380788856,
          0.9924162850967888,
          0.9923741744608525,
          0.9950979018007128,
          1.0008638974554627,
          0.9980535789745713,
          0.9870655038355484,
          0.9975057756253016,
          0.9745492601555213,
          0.9759142698947759,
          0.9853543836201333,
          0.9859126619219317,
          0.9894652209338839,
          0.9937417603373321,
          0.9977922721734677,
          1.000904101065611,
          1.0080315125807122,
          1.0157350792284927,
          1.0164672087775417,
          1.0087119108816616,
          1.013739019599561,
          1.0159087810828815,
          1.0209359872725914,
          1.017548013427483,
          1.0181689304747426,
          1.0116268536102921,
          1.0126517693686519,
          1.0094467762545163,
          1.02276910257205,
          1.0221963268566079,
          1.0193675489318827,
          1.027052902716174,
          1.027819846866083,
          1.0289475970231696,
          0.9944278060250974,
          1.0068638603917786,
          1.0057608073456923,
          0.9902654637881027,
          0.9880071074743658,
          0.9883560980336592,
          0.991416920962827,
          0.9654185655410665,
          0.9770562384534025,
          0.9783618294025855,
          0.9853422513765913,
          0.9809322227530703,
          0.9742684876652624,
          0.9861770201726163,
          0.9770869605224223,
          0.9604809005468127,
          0.9621688492793494,
          0.9522481197555812,
          0.9480798611420554,
          0.9458786678025728,
          0.9323688346909426,
          1.0255574267696905,
          1.0304448584033576,
          1.0352153962282866,
          1.0426561439794186,
          1.0454962090090685,
          1.0441133606103044,
          1.0401398343847017,
          1.049648823214946,
          1.0478259225121158,
          1.046970728004199,
          1.0567422034460692,
          1.0589131816108788,
          1.0691367135885073,
          1.067215577603974,
          1.0597936514881723,
          1.0535789407875966,
          1.0539832895313137,
          1.061877768455566,
          1.0724384256698087,
          0.9981271338637732,
          1.0061024255587752,
          1.005324841026978,
          0.9983346811566115,
          0.9959043227259924,
          0.9941156249646185,
          0.9962746254825642,
          1.0054522644749984,
          1.0041466069677674,
          1.0049944853938253,
          0.9943121928430123,
          0.9975441141160852,
          0.9955085710853512,
          0.9941160885814843,
          1.0054854384747183,
          1.015985864782046,
          1.0055370493437767,
          1.009497885131525,
          1.0050980912935759,
          1.0095482907843947,
          1.0008017681810133,
          0.9915111718284829,
          0.9637284057028116,
          0.9712256348790212,
          0.9744406796320255,
          0.9833714357441037,
          0.9733495155901238,
          0.9608830313053652,
          0.9717091902987729,
          0.939145328536987,
          0.9344081020896194,
          0.9503148352036818,
          0.962538775924862,
          0.9532929165942089,
          0.9611658328154943,
          0.9654775928775502,
          0.9387513940587586,
          0.9463102575630445,
          0.9403377993710271,
          0.9505784185721207,
          0.9629115660668879,
          0.9591892091886167,
          1.012458084984554,
          1.0294593790118247,
          1.0327705136928687,
          1.0450901063774218,
          1.0502485430419164,
          1.0613530705306546,
          1.061848748483993,
          1.061968380129056,
          1.063313908188821,
          1.0577282856388488,
          1.0564955144545127,
          1.0527872937838316,
          1.0510496813757793,
          1.0535657921021286,
          1.0466981577624925,
          1.0530228375719353,
          1.0478724394164542,
          1.0460887328114394,
          1.0529010826157763,
          0.9781119303195338,
          0.9848901066859216,
          0.9954792085662001,
          0.9918043293750888,
          0.9734513422167703,
          0.9807420945353238,
          0.9898132117278581,
          1.0076375053082187,
          1.0059891743198581,
          1.016993297171335,
          1.014816432737142,
          1.0178895535534478,
          1.0152040667078488,
          1.0224694019212088,
          1.023669516123842,
          1.028878994482253,
          1.024988454191492,
          1.0315553388203866,
          1.034590492866666,
          1.0360344773329777,
          1.0350980277411772,
          1.0311572726610132,
          1.004591419613152,
          1.007045553789953,
          1.0088703636270162,
          1.0169122117625187,
          1.0184831341157348,
          1.0149792985338266,
          1.0143958297211793,
          1.0130252537083886,
          1.0148492422417679,
          1.0235208514415925,
          1.0234696104833445,
          1.0152785563394526,
          1.010878044594261,
          1.0068410285173632,
          1.0142969147989025,
          1.0220308277200731,
          1.02181041736562,
          1.0259566013593306,
          1.0197999347030102,
          0.9866047233003026,
          0.9930056038589017,
          0.9971414775108041,
          1.007624356893399,
          1.0073427679776494,
          1.0053845565455957,
          1.007400567356719,
          1.0197707583345368,
          1.0133781127035566,
          1.0209332555616297,
          1.021848178010842,
          1.0237112196498672,
          1.0247164512380633,
          1.0295881247966587,
          1.033676306957228,
          1.0337057359989632,
          1.0357125850038207,
          1.034449871159386,
          1.0290464196826241
         ]
        },
        {
         "line": {
          "color": "rgba(50, 171, 96, 1.0)",
          "dash": "solid",
          "shape": "linear",
          "width": 1.3
         },
         "mode": "lines",
         "name": "Deep NNF",
         "text": "",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477
         ],
         "y": [
          1.0063282062208885,
          1.0081834408679298,
          1.0126113632419012,
          1.0132293315569558,
          1.0141735003933205,
          1.0092558190141088,
          1.0175593912828074,
          1.0215525320837868,
          1.0156054187813472,
          1.0226431388884127,
          1.0230104079258877,
          1.0291078890987841,
          1.0355798568921804,
          1.0377844560277234,
          1.0352070968297065,
          1.0368029457904417,
          1.0487435525859643,
          1.0421410864101566,
          1.032251066615339,
          1.0306895913934424,
          0.9785955505029192,
          0.9437299418689,
          0.9554549929871643,
          0.9503672167261065,
          0.9192282595919533,
          0.9322879113247667,
          0.9431872985465184,
          0.9461215213521601,
          0.9634020459203132,
          0.972849258231164,
          0.9726139521944591,
          0.966778331762045,
          0.9634186050491931,
          0.9640355823644369,
          0.9795653083613897,
          0.9851216940132714,
          0.972972903052823,
          0.9628241046207776,
          1.005736892143216,
          1.0183927497230676,
          1.0234247426646896,
          1.0222834277303605,
          1.0268416558032896,
          1.043742976959849,
          1.0408136502016692,
          1.0388483174322105,
          1.0349532457034443,
          1.0335197348804568,
          1.0367649153856655,
          1.028315752083218,
          1.0303271464072836,
          1.0279491704566153,
          1.006119686484999,
          0.9870704520379897,
          1.0098837312192046,
          0.9988181935359004,
          1.0011886706489195,
          1.014938167449161,
          1.0141303772633317,
          1.0261310377935056,
          1.0327683688847629,
          1.0112002913385658,
          1.0122133350534979,
          1.0289882556642336,
          1.023214052647084,
          1.0291874868751627,
          1.0264401523888702,
          1.0352297739113092,
          1.04345519850761,
          1.045777625725339,
          1.0382692787115626,
          1.0273559613834042,
          1.0285799652707193,
          1.0186580743740716,
          1.0212760681009432,
          1.0354451804540976,
          1.0392292387969988,
          1.030190708468326,
          0.9943065405457946,
          0.9898207526065526,
          1.003787900405988,
          1.0054788532547934,
          1.0058026616681233,
          1.0148937881033204,
          1.0239521692401312,
          1.020487646198299,
          1.0229753077902453,
          1.020362639120502,
          1.0255523402745859,
          1.0263227517162703,
          1.0211374511963218,
          1.0284434895271295,
          1.0277496507891828,
          1.0273806165935098,
          1.0288333757499577,
          1.0298025919687923,
          1.0196106101271427,
          1.029482157555895,
          1.0234066866500198,
          1.0080251379388694,
          1.009583864326694,
          1.0181160695617923,
          1.0182823732388493,
          1.0251813806565826,
          1.0266319872519891,
          1.0301402716078965,
          1.0269097830034435,
          1.030070425662168,
          1.0311177713788189,
          1.02876778268928,
          1.0223800812727653,
          1.0253984451121638,
          1.0201957855680122,
          1.0231146623028484,
          1.014416681478471,
          1.0143060028613367,
          1.0044446397590094,
          1.0122339958401796,
          1.0125225415945642,
          0.9962500380922638,
          1.004978012675665,
          1.0117389447377774,
          1.022229715637329,
          1.0277325396258934,
          1.0279847525029617,
          1.0364759698737052,
          1.035514047484625,
          1.0356655549486435,
          1.0383384684543353,
          1.0395059126353952,
          1.0367853310803763,
          1.0333433459856325,
          1.034477620517978,
          1.0319172387482936,
          1.0424180144793613,
          1.050574702481452,
          1.0434607547746069,
          1.0365962863532667,
          1.04447957860772,
          1.0051948797406034,
          1.0092331011555498,
          1.0099882984479682,
          1.010419568759068,
          1.0106002103874738,
          1.0112361839576183,
          0.9995828585929681,
          0.9958457825791485,
          1.0038801395088683,
          1.000689027447847,
          1.0088342547392055,
          1.0134132990535607,
          1.0159316775763347,
          1.0175481423305102,
          1.0132653198748027,
          1.0105781044172946,
          1.0157742306066584,
          1.0188257510658134,
          1.0188732623926502,
          1.0207357519057756,
          1.0152318838489622,
          1.0164572391155227,
          0.9990891364854609,
          0.9935432587835851,
          0.9896251848344051,
          0.9924706290300889,
          0.992496371666616,
          0.9963178299877196,
          1.0002956011331425,
          1.0039061848223214,
          0.9995569203285679,
          1.0024981984124226,
          1.001680116111555,
          1.0091982536717736,
          1.0115701245463025,
          1.0067671382203158,
          1.0050577813304504,
          0.9998006711067081,
          1.003275230855105,
          1.0064796473195992,
          1.0006765295494668,
          1.000229797026644,
          0.9941097718793923,
          0.9899404180096455,
          0.9902668497298783,
          0.9849302940793778,
          0.9568166535177265,
          0.9371520808700696,
          0.9434321657217704,
          0.9433342996548559,
          0.9626557648593237,
          0.9610766989638155,
          0.9471407341340486,
          0.9458705612673128,
          0.937364342861962,
          0.9293097635637495,
          0.9005918034357748,
          0.9099687057288103,
          0.900483210013109,
          0.9012079245172084,
          0.9193633631073227,
          0.9266444406589619,
          0.9970975749209776,
          1.0053614755364244,
          1.0126111206849095,
          1.030025803402222,
          1.0222885273566948,
          1.0125658262832775,
          0.9982074073391597,
          0.9968027232227918,
          0.9936025675850317,
          1.0057860995772119,
          1.0104155550647922,
          0.9964289459177351,
          0.9804341528764666,
          0.9866905127790875,
          0.9812561749790295,
          0.9971930073030626,
          0.9958845167406463,
          1.0145391718105854,
          1.012595030307499,
          1.0156174797926298,
          0.9659158814828082,
          0.9634523807200056,
          0.944243761498908,
          0.9403110984610981,
          0.9368688492119603,
          0.940708236990679,
          0.9359007220690042,
          0.9247174441319692,
          0.9041567833827089,
          0.9038416851183804,
          0.8876143544036776,
          0.8730179249935064,
          0.8580603213846724,
          0.8339487437170496,
          0.8689838506127295,
          0.8760132044747387,
          0.8757949424551223,
          0.8828935390482743,
          0.9809573057023003,
          1.0128829790497056,
          1.0272837867804927,
          1.03810351232306,
          1.0438737733648513,
          1.0493202867137952,
          1.0486734374802957,
          1.03816104117783,
          1.0486640322515366,
          1.05108911787143,
          1.0617769175557457,
          1.0744089704346895,
          1.0592345448661973,
          1.0584591888023955,
          1.0684618159448214,
          1.0816449158863597,
          1.0775440547126482,
          1.081051007711329,
          1.0914715256533574,
          1.1073314232675282,
          1.0059150720676469,
          1.0107136841767062,
          1.0095743761230824,
          1.006890554769749,
          1.006826210096436,
          1.0090220392343898,
          1.0189642978398188,
          1.021318467659937,
          1.0198518431155301,
          1.0315897167875077,
          1.0351540293567383,
          1.0375818627564286,
          1.0319212752814448,
          1.0370171843571516,
          1.037658532087647,
          1.0368094841963884,
          1.0352551745448495,
          1.0354774815988188,
          0.9969189355102981,
          0.9938907187001155,
          0.9864319578568151,
          0.9777415244184599,
          0.9760744040498047,
          0.9880338054108535,
          0.9904186231680439,
          0.994949636229363,
          0.9936137155359015,
          0.9973414312559021,
          1.0009745463185886,
          0.9979789500179647,
          0.9901415629829742,
          1.0025868440066985,
          0.9813408183425237,
          0.981720925497635,
          0.9893444172820107,
          0.9842791923969697,
          0.9889621792454835,
          0.9956577437870073,
          0.9989845703239376,
          1.0007074527330757,
          1.002903168472677,
          1.0080385262789422,
          1.0090317513622076,
          1.001107469856168,
          1.0064350255088985,
          1.008551395419492,
          1.014466164075604,
          1.0113136604493076,
          1.0114745814312023,
          1.0059552796578746,
          1.0051842053544688,
          1.004507583417271,
          1.0172369329441409,
          1.0139159602994707,
          1.0103524753734316,
          1.0175887681675757,
          1.0178910002942723,
          1.0242651472567423,
          1.0005806868647111,
          1.010613298236036,
          1.0038503813266317,
          0.9893582151416628,
          0.9860536594686083,
          0.9873037247216266,
          0.9912444107494796,
          0.9664041127032983,
          0.974356485833316,
          0.9774306894504359,
          0.9855171840317336,
          0.9786593637675381,
          0.9743940243295199,
          0.9837967131196744,
          0.9772839815458247,
          0.9647839182384264,
          0.9649904884021404,
          0.9533162222699675,
          0.944937199163378,
          0.9472749465810087,
          0.9388353438222556,
          1.0223870391938168,
          1.0331715415603893,
          1.0370607626404746,
          1.0438276063794125,
          1.0468923307867697,
          1.0452051356565295,
          1.0456109020353677,
          1.0493587416067773,
          1.044695093969935,
          1.0455982237498977,
          1.056130630952948,
          1.0597694965631617,
          1.0681017129577248,
          1.065190989897412,
          1.0615658066602986,
          1.0534356105891691,
          1.0496446504466048,
          1.0528759036904807,
          1.0640406865436267,
          1.0001447556685303,
          1.0089490744950036,
          1.0083589597199025,
          1.0020413140883189,
          1.002649041843542,
          1.002192197279399,
          1.0040513954585215,
          1.010986900981636,
          1.0086464310450043,
          1.006812577636486,
          1.0016559329407677,
          1.004243459271619,
          1.0004094350946096,
          1.0004640942629932,
          1.0071623094988995,
          1.0139795462447452,
          1.011064005207201,
          1.0161745615652027,
          1.0137128720728914,
          1.0112945131780466,
          1.0012627727788301,
          0.9963377659073055,
          0.9691069580393646,
          0.9818400896504684,
          0.9854501862970177,
          1.0018319365076673,
          0.9987546872603549,
          0.9906680764569509,
          1.0007746122248997,
          0.9723953365190687,
          0.9790659183836086,
          0.9914386535455052,
          1.0038633952943523,
          0.9951770906475347,
          1.0068468980047518,
          1.0080629972909028,
          0.9845018248340704,
          0.9942103545712471,
          0.9912361836520722,
          0.9973853034070829,
          1.0093286818751854,
          1.0102228308002448,
          1.013210754674651,
          1.0279502645522267,
          1.0303976728941744,
          1.033257754251104,
          1.0375077415932639,
          1.0414444671291856,
          1.0465499264836264,
          1.0472653975509207,
          1.051656132938901,
          1.0515872718544834,
          1.0490162172272384,
          1.0451625389061798,
          1.0393802132908645,
          1.0417212474385065,
          1.0340064154210122,
          1.0370421286697928,
          1.0355815454627946,
          1.0301211186691486,
          1.0346492304584822,
          0.9815526682444452,
          0.9891153655352019,
          1.0015756746424769,
          0.9972801448953906,
          0.9789849757740882,
          0.9862844944760902,
          0.9938824656785656,
          1.0101226987985599,
          1.0085820011801836,
          1.0173247258209657,
          1.0133381308601135,
          1.017848066269714,
          1.013723299230127,
          1.023338786607382,
          1.0238125922841137,
          1.0252827492825112,
          1.025760553352006,
          1.0303473660632338,
          1.0348669784779023,
          1.0353613907726495,
          1.0410177978679664,
          1.0358748019400443,
          1.0076958279255108,
          1.01037327181689,
          1.0092728324083915,
          1.0108242627962865,
          1.0080204380931963,
          1.0083274292597881,
          1.0102588839177966,
          1.007743546964952,
          1.010326806967007,
          1.0154601681945046,
          1.0148081807923015,
          1.0134380933063798,
          1.0103116372780834,
          1.0063836410027098,
          1.009456277202499,
          1.016764589349641,
          1.0197252308301215,
          1.0244910418435966,
          1.0186289531621924,
          0.9924014797115069,
          1.000305729399115,
          1.002965851048939,
          1.0124617607104989,
          1.0090622922048693,
          1.007377087772493,
          1.0119848686846906,
          1.023253565356379,
          1.0205291583631801,
          1.0285988891552582,
          1.0295806017564286,
          1.0312703717413056,
          1.0330266474583165,
          1.0365504761067434,
          1.037591097797218,
          1.0390316935897153,
          1.0412248884751119,
          1.0391554524812516,
          1.0345374123425317
         ]
        },
        {
         "line": {
          "color": "rgba(128, 0, 128, 1.0)",
          "dash": "solid",
          "shape": "linear",
          "width": 1.3
         },
         "mode": "lines",
         "name": "S&P 500",
         "text": "",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477
         ],
         "y": [
          1.0063988187678174,
          1.0104532331222378,
          1.01756052613646,
          1.0192520618530718,
          1.0205800775224427,
          1.0194449608291194,
          1.0266153766139654,
          1.0335446225887093,
          1.0299019074919185,
          1.0395984871573627,
          1.0379180905786511,
          1.0424696056080711,
          1.0508789228462483,
          1.053163917658637,
          1.0525741713615295,
          1.0532084745811838,
          1.0656797230238397,
          1.0585055944403243,
          1.0469691373757133,
          1.0474810899872826,
          0.9787914523050585,
          0.9386813371369133,
          0.9550528040953712,
          0.9502760228653357,
          0.9146060632223194,
          0.9282667019487502,
          0.9411831475856182,
          0.943642392884729,
          0.9562895208774654,
          0.9678310868101905,
          0.9681925422447539,
          0.9625369525123278,
          0.9572463650149637,
          0.9581782933130517,
          0.9735363356475689,
          0.9849822173437246,
          0.9724661579633176,
          0.9616758790755133,
          1.005071602697713,
          1.016159579134265,
          1.0188410806669994,
          1.0183480893579684,
          1.02289305656995,
          1.0406697424149502,
          1.0393439449479687,
          1.0327300001691544,
          1.0268181142903392,
          1.0260152139842424,
          1.0277629768289267,
          1.0131644306530774,
          1.014665743031788,
          1.0127947099523045,
          0.9873098694051805,
          0.9666090613837801,
          0.9928595108594567,
          0.9757065632079799,
          0.9728607617380555,
          0.9862567806817226,
          1.0126148657086849,
          1.0243272579849914,
          1.0313570764980473,
          1.0087494728739093,
          1.0121152146565606,
          1.029044819045906,
          1.0233589712662865,
          1.031802450431812,
          1.0288240233366426,
          1.037166796810276,
          1.0482245556115208,
          1.0490960136583551,
          1.0430887589823636,
          1.0341843981903012,
          1.0342425519413678,
          1.0204038059039324,
          1.022278348182939,
          1.0329450097814639,
          1.0340953231711592,
          1.0256286771649166,
          0.9927941364144521,
          0.990556701620733,
          1.0032469010248992,
          1.0067160741565893,
          1.0064486487434146,
          1.016193306164882,
          1.0257156914795584,
          1.0274671992820958,
          1.0283749557818398,
          1.0213386699391316,
          1.0254858786918764,
          1.0246081937600573,
          1.0219112252999667,
          1.0294598310819911,
          1.0262316900386654,
          1.029565311342211,
          1.0274822810205553,
          1.0250602786545306,
          1.0132062894955904,
          1.0260697452623868,
          1.0190108370003281,
          1.0044795984363044,
          1.0051853388746206,
          1.013797156235869,
          1.0130731138039089,
          1.016239883457275,
          1.0173259469223745,
          1.0190995380584336,
          1.01499651295076,
          1.0175051272030116,
          1.0164702200206917,
          1.0143090745060876,
          1.0102281010902103,
          1.0119577672952513,
          1.005536378857846,
          1.0074086217219176,
          0.9935822716687778,
          0.9957727005926209,
          0.9872047185704218,
          0.9933043504338407,
          0.9940576755436773,
          0.995052649459258,
          1.003630802740886,
          1.0121428782208493,
          1.0210730007304945,
          1.0246194600673186,
          1.01735060188897,
          1.0262514455236553,
          1.0273590147346074,
          1.0263027502102562,
          1.0303809679741733,
          1.0326071189351556,
          1.0285252300803842,
          1.0275497277211143,
          1.0294384148472329,
          1.03436006848548,
          1.0437744053116034,
          1.040609372314535,
          1.0337806764626405,
          1.0278321266601336,
          1.032852807699117,
          1.0049264450595978,
          1.0095935073980913,
          1.0131656786160577,
          1.016027043209936,
          1.0157604580692234,
          1.0142960621713257,
          1.0070804736124737,
          1.0030461173380103,
          1.0094548344287018,
          1.001780792294429,
          1.0097143035233918,
          1.013069701211911,
          1.015529452447731,
          1.0176301120772238,
          1.017224940696154,
          1.0155045466420984,
          1.021799496569033,
          1.0296371171228813,
          1.0299143763326282,
          1.0357863651188828,
          1.031197490780373,
          1.0313361637497624,
          0.9971968733321512,
          0.9935548060610239,
          0.9913557267355201,
          0.9932371481551123,
          0.9969516977518016,
          0.99730728234759,
          1.0025753131385442,
          1.0028515041435464,
          0.9972659000251012,
          1.0026202353268479,
          1.0038767917204379,
          1.0117477800203973,
          1.0113749176067663,
          1.0078192390796339,
          1.0065039383125098,
          1.003193266899322,
          1.0059653777973,
          1.0059584665320758,
          0.9996033098775927,
          1.0003145473287949,
          0.9921424950818614,
          0.9866579524562761,
          0.9862681077376338,
          0.9848696744950467,
          0.9525026920627381,
          0.9329068467389267,
          0.9461599060852729,
          0.9405728516576987,
          0.96079102966583,
          0.9605482739364328,
          0.9467241376357972,
          0.9463822093757982,
          0.9423132131602848,
          0.9371193427227399,
          0.908195684892166,
          0.925110865656466,
          0.9090812254028265,
          0.9031180167222123,
          0.9172669681153622,
          0.9272205363502554,
          0.9936833138368367,
          0.9992482555596338,
          1.005502845731112,
          1.026828411076269,
          1.0242521842534016,
          1.0148300745026697,
          0.994836410632192,
          0.9933621429867606,
          0.9858449635108176,
          0.996288761895005,
          0.9985038163368646,
          0.981885608556284,
          0.9640631667273433,
          0.9669970912180984,
          0.9606585777113843,
          0.9755798804019727,
          0.9787619217422666,
          1.0012479781394434,
          0.9990621456627131,
          1.007225230226082,
          0.9676350970612119,
          0.966162135472726,
          0.9436311197422418,
          0.9452939432407204,
          0.9449570911527936,
          0.950078289560467,
          0.9498883402068774,
          0.9317580973076341,
          0.9124022385020405,
          0.9124810706966155,
          0.8984327726729308,
          0.884262595477043,
          0.8660571951645509,
          0.8425764323077432,
          0.8843629509812443,
          0.8919354686452154,
          0.8908280571297416,
          0.8983934004766292,
          0.9752432698883858,
          1.0087289441747151,
          1.0158005727189647,
          1.025649048918211,
          1.029852205405627,
          1.034505509097238,
          1.034354163099138,
          1.028916019793164,
          1.0399477372149,
          1.0422584860637139,
          1.0501706874200907,
          1.0640151433024947,
          1.048951554993528,
          1.0512623038423419,
          1.052708552276846,
          1.0616446732558196,
          1.0533141306892309,
          1.0517802454545853,
          1.0681346509898666,
          1.0773178275788666,
          1.0067762366585589,
          1.0115165624123952,
          1.0092665064607713,
          0.9998226585351513,
          1.0004987397093463,
          1.001208196460029,
          1.0141139948903926,
          1.0171806702684842,
          1.0144834716703601,
          1.0255197866862462,
          1.0270567775769541,
          1.028881971070863,
          1.025253683967162,
          1.0318266902923765,
          1.0330977587686687,
          1.032281139342201,
          1.0317195276165916,
          1.0288043953566641,
          0.9961194417967204,
          0.9949922996852526,
          0.9885008718230445,
          0.9804685931210823,
          0.9783785388984994,
          0.9927274797038621,
          0.9956593199476047,
          1.0025787377178452,
          1.0017084767220341,
          1.0067018962137082,
          1.0104326800093903,
          1.01030075636313,
          1.0073260736501675,
          1.0182580610114618,
          0.9989371221273714,
          0.9980989930726437,
          1.0052680646971726,
          1.0005992731134172,
          1.0041909056447977,
          1.0109534084175678,
          1.0000174557671553,
          1.0021658701124747,
          1.004255020508249,
          1.0089111811654476,
          1.009967975470098,
          1.0038400002185275,
          1.0073312268920238,
          1.0073696293007466,
          1.0140276618667168,
          1.0133894641757184,
          1.013905642395667,
          1.0116002115954688,
          1.0131976216360477,
          1.0142229956295734,
          1.023189949870154,
          1.0209473596922052,
          1.0205706570592352,
          1.025352328759443,
          1.0264510163472285,
          1.0274275993632191,
          0.9978760145285372,
          1.0074938223262329,
          1.0029893290624603,
          0.9864283188695834,
          0.984844681860806,
          0.9818690476334619,
          0.9855218921413533,
          0.9617406998029281,
          0.9694499599446598,
          0.9751105541558941,
          0.9837844423649551,
          0.9780417646502367,
          0.9714405911041079,
          0.979693790669411,
          0.9769267475240655,
          0.9652874955299399,
          0.9665940693333113,
          0.9584981896994471,
          0.9518731343309619,
          0.953870612565939,
          0.9412839345034183,
          1.021432370802961,
          1.029769153185042,
          1.036087391196153,
          1.0469639233001995,
          1.0518428215271907,
          1.051474802791913,
          1.0493323396007523,
          1.0536318550631134,
          1.0519339144618276,
          1.052914053304957,
          1.063145640144341,
          1.0663193037765843,
          1.0764196778023158,
          1.0750642255017029,
          1.0732023358366578,
          1.0630107799695856,
          1.0616990949090916,
          1.065758157088725,
          1.0718942092305626,
          1.0029281260087797,
          1.010622970172487,
          1.0087978846193795,
          1.0039198984911424,
          1.00516130275557,
          1.0096952732130933,
          1.0120026559336488,
          1.0166782850421827,
          1.0168570873300702,
          1.0133959309372158,
          1.0067771953430888,
          1.0103834688412185,
          1.0041425983871153,
          1.0069830114917453,
          1.013878310416685,
          1.0186315219785724,
          1.0132710740588453,
          1.0207568075689852,
          1.0191071478241773,
          1.016479222190047,
          1.0054143110172231,
          0.9927172599946105,
          0.9631563039768206,
          0.9756937263621089,
          0.976441962374194,
          0.994762262933215,
          0.9881803124694816,
          0.9760085772476247,
          0.9907772347757092,
          0.9617546422813407,
          0.9641246634964737,
          0.9780332460813522,
          0.9898731847660052,
          0.982038608343735,
          0.9901372830014966,
          0.9896361992346404,
          0.9639587650585846,
          0.974545912560365,
          0.9714242658642347,
          0.977782704028637,
          0.9901881186022634,
          0.9908245989725447,
          1.0108420789476402,
          1.0239929461199893,
          1.0249253993956144,
          1.0248290459948386,
          1.0251593528807763,
          1.0325709281479636,
          1.0355438576901401,
          1.0347936951157761,
          1.0315490096821769,
          1.0342122137020153,
          1.0345666298412286,
          1.0345872951612392,
          1.0295223937932652,
          1.0294225961151402,
          1.0207585935184373,
          1.027045008364364,
          1.0245504018928016,
          1.0191035308549894,
          1.024247564581078,
          0.9820967968710144,
          0.9899259869058753,
          1.0039996632939374,
          0.9995034568489072,
          0.9839503644247936,
          0.992908732930873,
          0.9992789330839211,
          1.010210022957231,
          1.0088087414335511,
          1.018852115296318,
          1.016814876626137,
          1.0196241649519595,
          1.0156279061304307,
          1.0226069113170644,
          1.0189575682339933,
          1.0218586922880701,
          1.0238211169118265,
          1.0279908337726376,
          1.0337283979253455,
          1.0328679170138584,
          1.0362282186888858,
          1.033095845251253,
          1.0037040892383409,
          1.0025139978744835,
          1.0032182611433682,
          1.0059571427020124,
          1.0085330729466828,
          1.0065538599361377,
          1.0081287604511808,
          1.0088460788801936,
          1.009690526247189,
          1.0174605875413794,
          1.0179725256305474,
          1.0173692809793886,
          1.0135478544177075,
          1.0119436592697673,
          1.0141445716518327,
          1.0217580505833912,
          1.0240013923173887,
          1.0282760085846303,
          1.024151367378019,
          0.9933619048889829,
          0.999643496048875,
          1.001143212422562,
          1.0102893806729702,
          1.0070940158612913,
          1.0059893002274503,
          1.0089148760086188,
          1.017566548682094,
          1.0176404056482997,
          1.024914280649169,
          1.0252579266458848,
          1.0248147074530016,
          1.0293846552881123,
          1.0344747372133245,
          1.035370740866389,
          1.035168379503736,
          1.0404768953951842,
          1.040512255893812,
          1.0344972391152558
         ]
        }
       ],
       "layout": {
        "legend": {
         "bgcolor": "#F5F6F9",
         "font": {
          "color": "#4D5663"
         }
        },
        "paper_bgcolor": "#F5F6F9",
        "plot_bgcolor": "#F5F6F9",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "#4D5663"
         }
        },
        "xaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": "Days"
         },
         "zerolinecolor": "#E1E5ED"
        },
        "yaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": "Comulative Return"
         },
         "zerolinecolor": "#E1E5ED"
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"23534f6d-8c2d-4a42-9cd4-50e51c0b6324\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';                                    if (document.getElementById(\"23534f6d-8c2d-4a42-9cd4-50e51c0b6324\")) {                    Plotly.newPlot(                        \"23534f6d-8c2d-4a42-9cd4-50e51c0b6324\",                        [{\"line\":{\"color\":\"rgba(255, 153, 51, 1.0)\",\"dash\":\"solid\",\"shape\":\"linear\",\"width\":1.3},\"mode\":\"lines\",\"name\":\"1/N Model\",\"text\":\"\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477],\"y\":[1.0062676172526308,1.0081373676787257,1.0125131757629535,1.0131336025245914,1.0140962084113216,1.0092517024882537,1.017563373772811,1.0215155331543533,1.0155329744159463,1.022516909030883,1.022840681941257,1.029008509944538,1.0355318001044767,1.0377491798568295,1.0351488228943735,1.0368083968648878,1.0486690287323688,1.0421276975136546,1.0322581889519156,1.0307325062616097,0.978619685140137,0.9437409528062374,0.9553963794136182,0.950189399620171,0.9190094927117505,0.9320861637458316,0.9429556017907292,0.9458680755631698,0.9633203203322742,0.9728221736347036,0.9727024929545001,0.9669755507164752,0.963595341484127,0.9641432064223538,0.9796780397174724,0.9851835310350824,0.9729991737858936,0.9628906270422349,1.0058561493549647,1.0184317397609917,1.0234336834131754,1.0222334482799627,1.0268137235093575,1.0436970985510776,1.0407108524698367,1.0386850008934554,1.0347590778422104,1.0333204589726352,1.036653359449439,1.0281726949630379,1.030121724359793,1.0277271410963416,1.0058716124386446,0.9867416410238288,1.0095592204511217,0.9985920502285717,1.0009989702653261,1.0147689060356688,1.0141571500864428,1.026131685788871,1.032779724987422,1.0112011763090383,1.0122133854559046,1.029074374976798,1.0232267350920046,1.0292564792870862,1.0265231598857485,1.0352759950854056,1.0434903204486055,1.0456816564754212,1.0383068013932124,1.02751639789064,1.0287474098500906,1.0187573935751224,1.0212308807076882,1.0352590070883327,1.0389497326218964,1.029954704023546,0.9942960002645669,0.9898657629060906,1.0038104616503014,1.0054347795959102,1.0057615446546968,1.014785858409811,1.0238434412655386,1.0204267251734165,1.022948495579923,1.020438210776554,1.0256300029262129,1.0263616625928753,1.021210639286996,1.0284738729726632,1.0277868282158122,1.0273635993908525,1.0288381675414326,1.0298292320179487,1.0196770350745978,1.0295581633880195,1.0233082133531124,1.007966309140573,1.0095293918705008,1.0180393627359963,1.0182322438076858,1.025050768370751,1.0264984576393836,1.0299961895773504,1.0267544470127274,1.029844544162624,1.0308687383169326,1.0285242122089124,1.022093328912376,1.0251267079481519,1.0198579269434154,1.0227343296691052,1.014021791731769,1.013904967575662,1.0040517266112523,1.0118090476764858,1.0120817301772356,0.9962539892083371,1.0049408880972932,1.011687059475301,1.0220539008414642,1.0276651328464617,1.027954508891701,1.036463425687162,1.035441940257798,1.0356371252944525,1.0383587993359555,1.0395089062115708,1.0368624729161067,1.033457330024739,1.0346154098953289,1.0320465842272781,1.0425013350314924,1.0507075163748465,1.043657075631381,1.0367006157835106,1.0445596046811292,1.005141418981641,1.009076702450313,1.0098353511280198,1.0103496359625315,1.0104671286865077,1.011143804109723,0.9994437075470038,0.99565077183963,1.0036753223436372,1.0004977045308485,1.0086752548513287,1.0132425070919622,1.0156813113843062,1.017369625403544,1.0130682276849783,1.0103718914289335,1.0155414675533065,1.018651812358888,1.0186576456720056,1.0205113259977034,1.0150182196762532,1.0161885928490266,0.9990946895387253,0.9935830167713895,0.9896530236635233,0.9924345957378736,0.9925007260119059,0.9963079283315062,1.0002705672636791,1.0038353827272997,0.9995099418515249,1.0024199960020734,1.001544050729199,1.009050947033026,1.0114750191953645,1.006684313802626,1.004999435910754,0.9997550009809274,1.0032522490640254,1.0065134634831152,1.0007251579756498,1.0001884692615475,0.9940952457839094,0.9899338810261029,0.9902975176421791,0.9848562268658618,0.9567830249832061,0.9372297490763087,0.9435322458496118,0.943474269525047,0.9627190338342412,0.9611365287509082,0.9472195484676298,0.9458759354689725,0.9373696014297732,0.929321845291068,0.9006604689923742,0.9098925429488184,0.9003899238095467,0.9011832454474492,0.9194286663631426,0.9265486355274616,0.9971331933732974,1.0053941346904378,1.0124461393985802,1.0297915717056192,1.0220987611338177,1.0123949292191208,0.9980469028176246,0.9965810882228967,0.9933996582044836,1.0055508778693887,1.0101530317618495,0.9962709566260985,0.9802850858452985,0.9864833062200646,0.9810565989152972,0.9969474845407783,0.9956697248473115,1.014360074019292,1.012348202138001,1.0152853247552729,0.9658764291297905,0.9633113382011266,0.9440845169093361,0.9400731487719868,0.9366118158776926,0.9404229394899458,0.9356217218707389,0.9244951149643832,0.9039643651928988,0.9036705866450048,0.8874237031878657,0.872719266101339,0.857786986809128,0.8336787245797506,0.8687199131454718,0.8757482611409707,0.8754846192049873,0.8826026088439742,0.9811368762157169,1.0130389739521002,1.0275593356561288,1.0382738855649845,1.0440601833788623,1.049607418370916,1.0490909102581443,1.0385893876777463,1.0491449415342065,1.051537772847914,1.0621806235070839,1.0747218119690063,1.0595730433431079,1.0586762762009416,1.0686741090167586,1.0818147209894398,1.0777280296818272,1.081260409467504,1.0916081828954578,1.1074605177009977,1.0059244296140633,1.0106938122641282,1.0095333383816683,1.00696587973514,1.0069594542920088,1.0090972375944347,1.0189970875823562,1.0213393282819567,1.0198484045010903,1.0315401043995525,1.035043408098839,1.037273512775653,1.0317890940677794,1.0368888202749125,1.0374323451150729,1.0365868802633365,1.035089382745919,1.035374788007251,0.9968532345308485,0.993843837086334,0.9864356423450796,0.977678470104707,0.97600135487123,0.9879378394209413,0.9902514519701412,0.9948091381841477,0.9934528729188169,0.9971179182108871,1.0008500679179202,0.9978314641753808,0.9899179572358354,1.0022919886607855,0.9809984002583059,0.9813652760734167,0.9889604961177495,0.9838615277491429,0.9886530201638724,0.99537336193313,0.9989076780343974,1.0005716181260953,1.0027444377816077,1.0078888810082631,1.008888828988551,1.0009429941974748,1.006287757583334,1.0083735174744275,1.0143492054606784,1.0111914198954195,1.0113001195113003,1.0056939595033076,1.0049775964704293,1.0042849245617675,1.0169331346444892,1.0135943639528735,1.010069457230639,1.017263246156714,1.0175140565982,1.0238510242007028,1.000555275797696,1.01052842745075,1.0037316314443891,0.9892896852901018,0.9859766860597234,0.9871357186960947,0.991104410350258,0.9663733845093865,0.9743453140987016,0.9774782485136388,0.9855698358243612,0.9787028268272407,0.9743806111148651,0.9838457878230421,0.9772542122456191,0.9648302645980813,0.9650325464725124,0.953442733395369,0.945119522094652,0.9475208702508952,0.9390994015721559,1.022319882695113,1.0331545366112003,1.0370170137719463,1.0438001960661634,1.046792560930738,1.0450230968332916,1.0454868418490402,1.0492582563316795,1.0446737240808661,1.0455250702436847,1.0559653566851182,1.0596248476441676,1.0679251265719294,1.0649899645871956,1.0613133774881032,1.0532627710566616,1.0493568464340313,1.05247691726782,1.0636914431580637,1.000145405648481,1.009011334602456,1.0083849576089043,1.002023990064262,1.0025298619004357,1.002117402848222,1.0039034018125017,1.010840338908744,1.008572313962448,1.006717424657892,1.0014526519746791,1.0039751938039885,1.0000400906201974,1.0001137599711925,1.006943523740351,1.0137410933275368,1.0106979897593869,1.0158835492939478,1.0134320994666441,1.0110099869884708,1.0009000940204782,0.9963292848059946,0.9690685086718385,0.9817436934350928,0.9853433216909881,1.0017585885630758,0.9985168770684714,0.9903794809196037,1.0005181718212712,0.9721675295046693,0.9787988531520772,0.9911651847746658,1.0036836864776848,0.9949746051127623,1.0065877270589998,1.0077728349798338,0.9842156105638165,0.9939123276752477,0.9909719413999722,0.9971087500836917,1.009042283065375,1.009936502590018,1.0133287358757619,1.0280288016659285,1.0305455736840146,1.0336467929742261,1.0380895120680766,1.0419590366074942,1.0469649889675132,1.0478077857168906,1.0523226714094311,1.0521253995719648,1.0495621375885176,1.045606385105311,1.0397657863577192,1.0421054879456624,1.0343617767591833,1.0374282352061373,1.03589421502193,1.030526771734028,1.035100566041648,0.9815225204287752,0.9890941514117803,1.0015760222374903,0.997249584006487,0.9789639222326668,0.9862632333674558,0.9938979177940701,1.0101334133197464,1.0085534780165968,1.0172702926878983,1.0132389860049442,1.0177580742921404,1.0136030849445399,1.0232504735131966,1.0237031123228892,1.0251126132074186,1.0257007891919556,1.0303170167735902,1.034767867042786,1.035248535862385,1.040836178393598,1.0356722637784033,1.0076589773936924,1.0103524585683437,1.0094050914510302,1.0109835986940714,1.008241812574126,1.008504145984684,1.0103731408801766,1.0078978735398283,1.0105028334735733,1.0156419286655864,1.0150199137591145,1.0137234842694967,1.010589867447848,1.0066599248707033,1.0097055990155437,1.0169900855619807,1.0199149575539452,1.0246497637103096,1.018798935545428,0.9924317986670339,1.0003263822693502,1.0029581060714827,1.012401543130443,1.0090113905041393,1.007324862417747,1.0118676484032356,1.0230711671463713,1.0203533173625667,1.0283598452002778,1.0293095669439074,1.0309142246132466,1.0326325383702042,1.0361774607696423,1.0371417781028276,1.0385741874477286,1.0408030030668383,1.0387410442370348,1.0341601551671005],\"type\":\"scatter\"},{\"line\":{\"color\":\"rgba(55, 128, 191, 1.0)\",\"dash\":\"solid\",\"shape\":\"linear\",\"width\":1.3},\"mode\":\"lines\",\"name\":\"Shallow NNF\",\"text\":\"\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477],\"y\":[1.0069168371659327,1.0107877253501658,1.0165329356155375,1.0183455311718637,1.0180295594827031,1.0152280800195925,1.0278416087379323,1.0324325204989835,1.0240530249492332,1.0341570402497788,1.032651235252834,1.0418657001082332,1.0454584149203885,1.0441199288837293,1.0452178467165403,1.0451194771311692,1.0545796218715104,1.0449400265030868,1.0300290650784671,1.027886672909397,0.9811419076458749,0.944354690145757,0.9570079052121487,0.958733320115237,0.9195170641893557,0.9314111101095875,0.9425661313694746,0.9451629903765766,0.962752073099184,0.9729429601769003,0.9721457276982586,0.966885570636071,0.96175008853746,0.9581807179436092,0.9727129275340676,0.9772682585779106,0.966483938568018,0.957012466570945,1.0107754587216884,1.021090976156776,1.0247500552558866,1.0221879164001875,1.0241956250929665,1.0401193972560936,1.0383236219427787,1.0346633940539234,1.0274771067248853,1.0237125969286653,1.027619385364597,1.0171907450948088,1.0137805149650678,1.0130735726750308,0.988596239504198,0.9688764127782801,0.9914461028710935,0.9823063067870893,0.9821949561286345,0.9951486330363923,1.0125215494116369,1.0300068005745007,1.0384177953419151,1.0154112123636996,1.0151864681721228,1.028597303004924,1.0229672168748307,1.026245266579425,1.0197233783138093,1.0283186342729846,1.0338395832115435,1.034690781117092,1.0292837536125743,1.0234206489058966,1.0282742908606344,1.0186658041875176,1.0215497783477883,1.0304274578489268,1.03870961380563,1.0264752670179056,0.9891075132307026,0.9792968990031755,0.991671575084308,0.9915640926210123,0.9881937696104643,0.992908944147673,1.0018540388931634,1.005312438740033,1.0054077627519746,1.0020715050753097,1.0114815434093423,1.0149423715488652,1.0109118511073867,1.0155239808033645,1.0103782638655592,1.0108716573756071,1.008585864301332,1.0086071024108676,0.997327659158928,1.0108148004807833,0.9998142958905771,1.0073188467661385,1.0089731737583147,1.0180611035968032,1.0170228846968739,1.0242999451927985,1.0273638250618864,1.0302295226052414,1.024406498835294,1.027881261000316,1.0310545625378889,1.028110896095943,1.0276502292361798,1.0303297313417832,1.0227881712649365,1.021939136169062,1.0104164952453212,1.0089488338382777,0.9988789542057858,1.0021239528057926,1.0029619686056772,1.0001287238890688,1.0078488524570224,1.0158463481140878,1.0270934034592119,1.031122380756486,1.02265547968936,1.0271589691644705,1.0293836554487832,1.0277409119309902,1.0337019312652658,1.0361037954347063,1.0336576793545071,1.0302653950504126,1.031088746067788,1.027088964852801,1.0306980874983591,1.0374257058705385,1.030998213480477,1.030585808759985,1.0397198749358403,1.005425838754541,1.011014907651121,1.0136009073516994,1.0130472448159147,1.0130557509166,1.013753572014087,1.0052107169532007,1.0001979040889233,1.0087564141630643,0.9949867788074975,1.005844318556182,1.0130973630036164,1.020041187243813,1.0271119722519952,1.021066481807283,1.0162813331919474,1.0196724460199271,1.0262709672490344,1.023697548812268,1.0262970339999302,1.0197146102139463,1.0208645453889729,1.0004979197170747,0.9971441309835651,0.9949792531865499,0.9983276601831503,1.0013546641669517,1.0037934148108851,1.0071775930152147,1.002898730629677,1.000805777712539,1.0051746178402308,1.0037229595300912,1.0097105490163651,1.0103595528446019,1.0012395822669236,0.9944953088845104,0.9911009090505815,0.9902513751703731,0.988425241259303,0.9986656553695232,1.0013041861490166,0.9939260850935121,0.9885040476664348,0.9895816946943454,0.9820156064320315,0.9562267994383857,0.9319170783747353,0.938559030432152,0.9402136930922951,0.9577066848284614,0.9537994701407709,0.9401401634362694,0.9373404483201486,0.9294503491470876,0.9257622813296706,0.8905378785663388,0.9028727087073569,0.8922592459708599,0.8891357294414756,0.9096894390880926,0.912876757973452,0.9942313544365353,1.0049911913149878,1.0095269484971436,1.0210524043974467,1.0121185471662286,1.005561024523369,0.9934674210461937,0.9914435585494332,0.9839967739082662,0.9929479977356533,0.9958642348646242,0.9859792797561567,0.9674226635390779,0.9767069685497112,0.969767466282881,0.9832454187623141,0.9819766250500234,1.0001727895401875,0.9934905012984074,1.0020999731877869,0.9667564796818849,0.9635213896060955,0.9422461022433173,0.9379066528942326,0.9345157576451625,0.9426187138011975,0.9358164093359088,0.9222134041795107,0.9059610386173792,0.9045963169417646,0.8919765153846672,0.8764846376945427,0.8552815012139589,0.8339853425058167,0.8737791497624572,0.881910730245253,0.8781881612710182,0.8840353902127064,0.9840869285163731,1.0201408390197506,1.030731626263048,1.0429935752844128,1.0501529491207482,1.0506793333116156,1.052336126948381,1.0509521151621972,1.0571551322496875,1.0589135888222327,1.0688840875274657,1.0849598477069637,1.0685085794127878,1.06947814010204,1.0763750831808174,1.0881516858078062,1.0845841464320762,1.0867570430741365,1.0899442007639957,1.0981260416485894,1.0050133106863002,1.0103100050947285,1.0059545417151363,1.0054685979171447,1.007789086346224,1.011602127555183,1.0277024478820098,1.0341786296746913,1.0250948483650961,1.039203429461427,1.040482081121944,1.043342222711725,1.0392956069623212,1.0391865318623061,1.0398583958977599,1.0359009326503006,1.0371216277882953,1.0365015138502605,0.9963508149137158,0.9924485103723099,0.9823749493641015,0.9746984142501012,0.9725525439116689,0.9846026940356761,0.9871309380788856,0.9924162850967888,0.9923741744608525,0.9950979018007128,1.0008638974554627,0.9980535789745713,0.9870655038355484,0.9975057756253016,0.9745492601555213,0.9759142698947759,0.9853543836201333,0.9859126619219317,0.9894652209338839,0.9937417603373321,0.9977922721734677,1.000904101065611,1.0080315125807122,1.0157350792284927,1.0164672087775417,1.0087119108816616,1.013739019599561,1.0159087810828815,1.0209359872725914,1.017548013427483,1.0181689304747426,1.0116268536102921,1.0126517693686519,1.0094467762545163,1.02276910257205,1.0221963268566079,1.0193675489318827,1.027052902716174,1.027819846866083,1.0289475970231696,0.9944278060250974,1.0068638603917786,1.0057608073456923,0.9902654637881027,0.9880071074743658,0.9883560980336592,0.991416920962827,0.9654185655410665,0.9770562384534025,0.9783618294025855,0.9853422513765913,0.9809322227530703,0.9742684876652624,0.9861770201726163,0.9770869605224223,0.9604809005468127,0.9621688492793494,0.9522481197555812,0.9480798611420554,0.9458786678025728,0.9323688346909426,1.0255574267696905,1.0304448584033576,1.0352153962282866,1.0426561439794186,1.0454962090090685,1.0441133606103044,1.0401398343847017,1.049648823214946,1.0478259225121158,1.046970728004199,1.0567422034460692,1.0589131816108788,1.0691367135885073,1.067215577603974,1.0597936514881723,1.0535789407875966,1.0539832895313137,1.061877768455566,1.0724384256698087,0.9981271338637732,1.0061024255587752,1.005324841026978,0.9983346811566115,0.9959043227259924,0.9941156249646185,0.9962746254825642,1.0054522644749984,1.0041466069677674,1.0049944853938253,0.9943121928430123,0.9975441141160852,0.9955085710853512,0.9941160885814843,1.0054854384747183,1.015985864782046,1.0055370493437767,1.009497885131525,1.0050980912935759,1.0095482907843947,1.0008017681810133,0.9915111718284829,0.9637284057028116,0.9712256348790212,0.9744406796320255,0.9833714357441037,0.9733495155901238,0.9608830313053652,0.9717091902987729,0.939145328536987,0.9344081020896194,0.9503148352036818,0.962538775924862,0.9532929165942089,0.9611658328154943,0.9654775928775502,0.9387513940587586,0.9463102575630445,0.9403377993710271,0.9505784185721207,0.9629115660668879,0.9591892091886167,1.012458084984554,1.0294593790118247,1.0327705136928687,1.0450901063774218,1.0502485430419164,1.0613530705306546,1.061848748483993,1.061968380129056,1.063313908188821,1.0577282856388488,1.0564955144545127,1.0527872937838316,1.0510496813757793,1.0535657921021286,1.0466981577624925,1.0530228375719353,1.0478724394164542,1.0460887328114394,1.0529010826157763,0.9781119303195338,0.9848901066859216,0.9954792085662001,0.9918043293750888,0.9734513422167703,0.9807420945353238,0.9898132117278581,1.0076375053082187,1.0059891743198581,1.016993297171335,1.014816432737142,1.0178895535534478,1.0152040667078488,1.0224694019212088,1.023669516123842,1.028878994482253,1.024988454191492,1.0315553388203866,1.034590492866666,1.0360344773329777,1.0350980277411772,1.0311572726610132,1.004591419613152,1.007045553789953,1.0088703636270162,1.0169122117625187,1.0184831341157348,1.0149792985338266,1.0143958297211793,1.0130252537083886,1.0148492422417679,1.0235208514415925,1.0234696104833445,1.0152785563394526,1.010878044594261,1.0068410285173632,1.0142969147989025,1.0220308277200731,1.02181041736562,1.0259566013593306,1.0197999347030102,0.9866047233003026,0.9930056038589017,0.9971414775108041,1.007624356893399,1.0073427679776494,1.0053845565455957,1.007400567356719,1.0197707583345368,1.0133781127035566,1.0209332555616297,1.021848178010842,1.0237112196498672,1.0247164512380633,1.0295881247966587,1.033676306957228,1.0337057359989632,1.0357125850038207,1.034449871159386,1.0290464196826241],\"type\":\"scatter\"},{\"line\":{\"color\":\"rgba(50, 171, 96, 1.0)\",\"dash\":\"solid\",\"shape\":\"linear\",\"width\":1.3},\"mode\":\"lines\",\"name\":\"Deep NNF\",\"text\":\"\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477],\"y\":[1.0063282062208885,1.0081834408679298,1.0126113632419012,1.0132293315569558,1.0141735003933205,1.0092558190141088,1.0175593912828074,1.0215525320837868,1.0156054187813472,1.0226431388884127,1.0230104079258877,1.0291078890987841,1.0355798568921804,1.0377844560277234,1.0352070968297065,1.0368029457904417,1.0487435525859643,1.0421410864101566,1.032251066615339,1.0306895913934424,0.9785955505029192,0.9437299418689,0.9554549929871643,0.9503672167261065,0.9192282595919533,0.9322879113247667,0.9431872985465184,0.9461215213521601,0.9634020459203132,0.972849258231164,0.9726139521944591,0.966778331762045,0.9634186050491931,0.9640355823644369,0.9795653083613897,0.9851216940132714,0.972972903052823,0.9628241046207776,1.005736892143216,1.0183927497230676,1.0234247426646896,1.0222834277303605,1.0268416558032896,1.043742976959849,1.0408136502016692,1.0388483174322105,1.0349532457034443,1.0335197348804568,1.0367649153856655,1.028315752083218,1.0303271464072836,1.0279491704566153,1.006119686484999,0.9870704520379897,1.0098837312192046,0.9988181935359004,1.0011886706489195,1.014938167449161,1.0141303772633317,1.0261310377935056,1.0327683688847629,1.0112002913385658,1.0122133350534979,1.0289882556642336,1.023214052647084,1.0291874868751627,1.0264401523888702,1.0352297739113092,1.04345519850761,1.045777625725339,1.0382692787115626,1.0273559613834042,1.0285799652707193,1.0186580743740716,1.0212760681009432,1.0354451804540976,1.0392292387969988,1.030190708468326,0.9943065405457946,0.9898207526065526,1.003787900405988,1.0054788532547934,1.0058026616681233,1.0148937881033204,1.0239521692401312,1.020487646198299,1.0229753077902453,1.020362639120502,1.0255523402745859,1.0263227517162703,1.0211374511963218,1.0284434895271295,1.0277496507891828,1.0273806165935098,1.0288333757499577,1.0298025919687923,1.0196106101271427,1.029482157555895,1.0234066866500198,1.0080251379388694,1.009583864326694,1.0181160695617923,1.0182823732388493,1.0251813806565826,1.0266319872519891,1.0301402716078965,1.0269097830034435,1.030070425662168,1.0311177713788189,1.02876778268928,1.0223800812727653,1.0253984451121638,1.0201957855680122,1.0231146623028484,1.014416681478471,1.0143060028613367,1.0044446397590094,1.0122339958401796,1.0125225415945642,0.9962500380922638,1.004978012675665,1.0117389447377774,1.022229715637329,1.0277325396258934,1.0279847525029617,1.0364759698737052,1.035514047484625,1.0356655549486435,1.0383384684543353,1.0395059126353952,1.0367853310803763,1.0333433459856325,1.034477620517978,1.0319172387482936,1.0424180144793613,1.050574702481452,1.0434607547746069,1.0365962863532667,1.04447957860772,1.0051948797406034,1.0092331011555498,1.0099882984479682,1.010419568759068,1.0106002103874738,1.0112361839576183,0.9995828585929681,0.9958457825791485,1.0038801395088683,1.000689027447847,1.0088342547392055,1.0134132990535607,1.0159316775763347,1.0175481423305102,1.0132653198748027,1.0105781044172946,1.0157742306066584,1.0188257510658134,1.0188732623926502,1.0207357519057756,1.0152318838489622,1.0164572391155227,0.9990891364854609,0.9935432587835851,0.9896251848344051,0.9924706290300889,0.992496371666616,0.9963178299877196,1.0002956011331425,1.0039061848223214,0.9995569203285679,1.0024981984124226,1.001680116111555,1.0091982536717736,1.0115701245463025,1.0067671382203158,1.0050577813304504,0.9998006711067081,1.003275230855105,1.0064796473195992,1.0006765295494668,1.000229797026644,0.9941097718793923,0.9899404180096455,0.9902668497298783,0.9849302940793778,0.9568166535177265,0.9371520808700696,0.9434321657217704,0.9433342996548559,0.9626557648593237,0.9610766989638155,0.9471407341340486,0.9458705612673128,0.937364342861962,0.9293097635637495,0.9005918034357748,0.9099687057288103,0.900483210013109,0.9012079245172084,0.9193633631073227,0.9266444406589619,0.9970975749209776,1.0053614755364244,1.0126111206849095,1.030025803402222,1.0222885273566948,1.0125658262832775,0.9982074073391597,0.9968027232227918,0.9936025675850317,1.0057860995772119,1.0104155550647922,0.9964289459177351,0.9804341528764666,0.9866905127790875,0.9812561749790295,0.9971930073030626,0.9958845167406463,1.0145391718105854,1.012595030307499,1.0156174797926298,0.9659158814828082,0.9634523807200056,0.944243761498908,0.9403110984610981,0.9368688492119603,0.940708236990679,0.9359007220690042,0.9247174441319692,0.9041567833827089,0.9038416851183804,0.8876143544036776,0.8730179249935064,0.8580603213846724,0.8339487437170496,0.8689838506127295,0.8760132044747387,0.8757949424551223,0.8828935390482743,0.9809573057023003,1.0128829790497056,1.0272837867804927,1.03810351232306,1.0438737733648513,1.0493202867137952,1.0486734374802957,1.03816104117783,1.0486640322515366,1.05108911787143,1.0617769175557457,1.0744089704346895,1.0592345448661973,1.0584591888023955,1.0684618159448214,1.0816449158863597,1.0775440547126482,1.081051007711329,1.0914715256533574,1.1073314232675282,1.0059150720676469,1.0107136841767062,1.0095743761230824,1.006890554769749,1.006826210096436,1.0090220392343898,1.0189642978398188,1.021318467659937,1.0198518431155301,1.0315897167875077,1.0351540293567383,1.0375818627564286,1.0319212752814448,1.0370171843571516,1.037658532087647,1.0368094841963884,1.0352551745448495,1.0354774815988188,0.9969189355102981,0.9938907187001155,0.9864319578568151,0.9777415244184599,0.9760744040498047,0.9880338054108535,0.9904186231680439,0.994949636229363,0.9936137155359015,0.9973414312559021,1.0009745463185886,0.9979789500179647,0.9901415629829742,1.0025868440066985,0.9813408183425237,0.981720925497635,0.9893444172820107,0.9842791923969697,0.9889621792454835,0.9956577437870073,0.9989845703239376,1.0007074527330757,1.002903168472677,1.0080385262789422,1.0090317513622076,1.001107469856168,1.0064350255088985,1.008551395419492,1.014466164075604,1.0113136604493076,1.0114745814312023,1.0059552796578746,1.0051842053544688,1.004507583417271,1.0172369329441409,1.0139159602994707,1.0103524753734316,1.0175887681675757,1.0178910002942723,1.0242651472567423,1.0005806868647111,1.010613298236036,1.0038503813266317,0.9893582151416628,0.9860536594686083,0.9873037247216266,0.9912444107494796,0.9664041127032983,0.974356485833316,0.9774306894504359,0.9855171840317336,0.9786593637675381,0.9743940243295199,0.9837967131196744,0.9772839815458247,0.9647839182384264,0.9649904884021404,0.9533162222699675,0.944937199163378,0.9472749465810087,0.9388353438222556,1.0223870391938168,1.0331715415603893,1.0370607626404746,1.0438276063794125,1.0468923307867697,1.0452051356565295,1.0456109020353677,1.0493587416067773,1.044695093969935,1.0455982237498977,1.056130630952948,1.0597694965631617,1.0681017129577248,1.065190989897412,1.0615658066602986,1.0534356105891691,1.0496446504466048,1.0528759036904807,1.0640406865436267,1.0001447556685303,1.0089490744950036,1.0083589597199025,1.0020413140883189,1.002649041843542,1.002192197279399,1.0040513954585215,1.010986900981636,1.0086464310450043,1.006812577636486,1.0016559329407677,1.004243459271619,1.0004094350946096,1.0004640942629932,1.0071623094988995,1.0139795462447452,1.011064005207201,1.0161745615652027,1.0137128720728914,1.0112945131780466,1.0012627727788301,0.9963377659073055,0.9691069580393646,0.9818400896504684,0.9854501862970177,1.0018319365076673,0.9987546872603549,0.9906680764569509,1.0007746122248997,0.9723953365190687,0.9790659183836086,0.9914386535455052,1.0038633952943523,0.9951770906475347,1.0068468980047518,1.0080629972909028,0.9845018248340704,0.9942103545712471,0.9912361836520722,0.9973853034070829,1.0093286818751854,1.0102228308002448,1.013210754674651,1.0279502645522267,1.0303976728941744,1.033257754251104,1.0375077415932639,1.0414444671291856,1.0465499264836264,1.0472653975509207,1.051656132938901,1.0515872718544834,1.0490162172272384,1.0451625389061798,1.0393802132908645,1.0417212474385065,1.0340064154210122,1.0370421286697928,1.0355815454627946,1.0301211186691486,1.0346492304584822,0.9815526682444452,0.9891153655352019,1.0015756746424769,0.9972801448953906,0.9789849757740882,0.9862844944760902,0.9938824656785656,1.0101226987985599,1.0085820011801836,1.0173247258209657,1.0133381308601135,1.017848066269714,1.013723299230127,1.023338786607382,1.0238125922841137,1.0252827492825112,1.025760553352006,1.0303473660632338,1.0348669784779023,1.0353613907726495,1.0410177978679664,1.0358748019400443,1.0076958279255108,1.01037327181689,1.0092728324083915,1.0108242627962865,1.0080204380931963,1.0083274292597881,1.0102588839177966,1.007743546964952,1.010326806967007,1.0154601681945046,1.0148081807923015,1.0134380933063798,1.0103116372780834,1.0063836410027098,1.009456277202499,1.016764589349641,1.0197252308301215,1.0244910418435966,1.0186289531621924,0.9924014797115069,1.000305729399115,1.002965851048939,1.0124617607104989,1.0090622922048693,1.007377087772493,1.0119848686846906,1.023253565356379,1.0205291583631801,1.0285988891552582,1.0295806017564286,1.0312703717413056,1.0330266474583165,1.0365504761067434,1.037591097797218,1.0390316935897153,1.0412248884751119,1.0391554524812516,1.0345374123425317],\"type\":\"scatter\"},{\"line\":{\"color\":\"rgba(128, 0, 128, 1.0)\",\"dash\":\"solid\",\"shape\":\"linear\",\"width\":1.3},\"mode\":\"lines\",\"name\":\"S&P 500\",\"text\":\"\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477],\"y\":[1.0063988187678174,1.0104532331222378,1.01756052613646,1.0192520618530718,1.0205800775224427,1.0194449608291194,1.0266153766139654,1.0335446225887093,1.0299019074919185,1.0395984871573627,1.0379180905786511,1.0424696056080711,1.0508789228462483,1.053163917658637,1.0525741713615295,1.0532084745811838,1.0656797230238397,1.0585055944403243,1.0469691373757133,1.0474810899872826,0.9787914523050585,0.9386813371369133,0.9550528040953712,0.9502760228653357,0.9146060632223194,0.9282667019487502,0.9411831475856182,0.943642392884729,0.9562895208774654,0.9678310868101905,0.9681925422447539,0.9625369525123278,0.9572463650149637,0.9581782933130517,0.9735363356475689,0.9849822173437246,0.9724661579633176,0.9616758790755133,1.005071602697713,1.016159579134265,1.0188410806669994,1.0183480893579684,1.02289305656995,1.0406697424149502,1.0393439449479687,1.0327300001691544,1.0268181142903392,1.0260152139842424,1.0277629768289267,1.0131644306530774,1.014665743031788,1.0127947099523045,0.9873098694051805,0.9666090613837801,0.9928595108594567,0.9757065632079799,0.9728607617380555,0.9862567806817226,1.0126148657086849,1.0243272579849914,1.0313570764980473,1.0087494728739093,1.0121152146565606,1.029044819045906,1.0233589712662865,1.031802450431812,1.0288240233366426,1.037166796810276,1.0482245556115208,1.0490960136583551,1.0430887589823636,1.0341843981903012,1.0342425519413678,1.0204038059039324,1.022278348182939,1.0329450097814639,1.0340953231711592,1.0256286771649166,0.9927941364144521,0.990556701620733,1.0032469010248992,1.0067160741565893,1.0064486487434146,1.016193306164882,1.0257156914795584,1.0274671992820958,1.0283749557818398,1.0213386699391316,1.0254858786918764,1.0246081937600573,1.0219112252999667,1.0294598310819911,1.0262316900386654,1.029565311342211,1.0274822810205553,1.0250602786545306,1.0132062894955904,1.0260697452623868,1.0190108370003281,1.0044795984363044,1.0051853388746206,1.013797156235869,1.0130731138039089,1.016239883457275,1.0173259469223745,1.0190995380584336,1.01499651295076,1.0175051272030116,1.0164702200206917,1.0143090745060876,1.0102281010902103,1.0119577672952513,1.005536378857846,1.0074086217219176,0.9935822716687778,0.9957727005926209,0.9872047185704218,0.9933043504338407,0.9940576755436773,0.995052649459258,1.003630802740886,1.0121428782208493,1.0210730007304945,1.0246194600673186,1.01735060188897,1.0262514455236553,1.0273590147346074,1.0263027502102562,1.0303809679741733,1.0326071189351556,1.0285252300803842,1.0275497277211143,1.0294384148472329,1.03436006848548,1.0437744053116034,1.040609372314535,1.0337806764626405,1.0278321266601336,1.032852807699117,1.0049264450595978,1.0095935073980913,1.0131656786160577,1.016027043209936,1.0157604580692234,1.0142960621713257,1.0070804736124737,1.0030461173380103,1.0094548344287018,1.001780792294429,1.0097143035233918,1.013069701211911,1.015529452447731,1.0176301120772238,1.017224940696154,1.0155045466420984,1.021799496569033,1.0296371171228813,1.0299143763326282,1.0357863651188828,1.031197490780373,1.0313361637497624,0.9971968733321512,0.9935548060610239,0.9913557267355201,0.9932371481551123,0.9969516977518016,0.99730728234759,1.0025753131385442,1.0028515041435464,0.9972659000251012,1.0026202353268479,1.0038767917204379,1.0117477800203973,1.0113749176067663,1.0078192390796339,1.0065039383125098,1.003193266899322,1.0059653777973,1.0059584665320758,0.9996033098775927,1.0003145473287949,0.9921424950818614,0.9866579524562761,0.9862681077376338,0.9848696744950467,0.9525026920627381,0.9329068467389267,0.9461599060852729,0.9405728516576987,0.96079102966583,0.9605482739364328,0.9467241376357972,0.9463822093757982,0.9423132131602848,0.9371193427227399,0.908195684892166,0.925110865656466,0.9090812254028265,0.9031180167222123,0.9172669681153622,0.9272205363502554,0.9936833138368367,0.9992482555596338,1.005502845731112,1.026828411076269,1.0242521842534016,1.0148300745026697,0.994836410632192,0.9933621429867606,0.9858449635108176,0.996288761895005,0.9985038163368646,0.981885608556284,0.9640631667273433,0.9669970912180984,0.9606585777113843,0.9755798804019727,0.9787619217422666,1.0012479781394434,0.9990621456627131,1.007225230226082,0.9676350970612119,0.966162135472726,0.9436311197422418,0.9452939432407204,0.9449570911527936,0.950078289560467,0.9498883402068774,0.9317580973076341,0.9124022385020405,0.9124810706966155,0.8984327726729308,0.884262595477043,0.8660571951645509,0.8425764323077432,0.8843629509812443,0.8919354686452154,0.8908280571297416,0.8983934004766292,0.9752432698883858,1.0087289441747151,1.0158005727189647,1.025649048918211,1.029852205405627,1.034505509097238,1.034354163099138,1.028916019793164,1.0399477372149,1.0422584860637139,1.0501706874200907,1.0640151433024947,1.048951554993528,1.0512623038423419,1.052708552276846,1.0616446732558196,1.0533141306892309,1.0517802454545853,1.0681346509898666,1.0773178275788666,1.0067762366585589,1.0115165624123952,1.0092665064607713,0.9998226585351513,1.0004987397093463,1.001208196460029,1.0141139948903926,1.0171806702684842,1.0144834716703601,1.0255197866862462,1.0270567775769541,1.028881971070863,1.025253683967162,1.0318266902923765,1.0330977587686687,1.032281139342201,1.0317195276165916,1.0288043953566641,0.9961194417967204,0.9949922996852526,0.9885008718230445,0.9804685931210823,0.9783785388984994,0.9927274797038621,0.9956593199476047,1.0025787377178452,1.0017084767220341,1.0067018962137082,1.0104326800093903,1.01030075636313,1.0073260736501675,1.0182580610114618,0.9989371221273714,0.9980989930726437,1.0052680646971726,1.0005992731134172,1.0041909056447977,1.0109534084175678,1.0000174557671553,1.0021658701124747,1.004255020508249,1.0089111811654476,1.009967975470098,1.0038400002185275,1.0073312268920238,1.0073696293007466,1.0140276618667168,1.0133894641757184,1.013905642395667,1.0116002115954688,1.0131976216360477,1.0142229956295734,1.023189949870154,1.0209473596922052,1.0205706570592352,1.025352328759443,1.0264510163472285,1.0274275993632191,0.9978760145285372,1.0074938223262329,1.0029893290624603,0.9864283188695834,0.984844681860806,0.9818690476334619,0.9855218921413533,0.9617406998029281,0.9694499599446598,0.9751105541558941,0.9837844423649551,0.9780417646502367,0.9714405911041079,0.979693790669411,0.9769267475240655,0.9652874955299399,0.9665940693333113,0.9584981896994471,0.9518731343309619,0.953870612565939,0.9412839345034183,1.021432370802961,1.029769153185042,1.036087391196153,1.0469639233001995,1.0518428215271907,1.051474802791913,1.0493323396007523,1.0536318550631134,1.0519339144618276,1.052914053304957,1.063145640144341,1.0663193037765843,1.0764196778023158,1.0750642255017029,1.0732023358366578,1.0630107799695856,1.0616990949090916,1.065758157088725,1.0718942092305626,1.0029281260087797,1.010622970172487,1.0087978846193795,1.0039198984911424,1.00516130275557,1.0096952732130933,1.0120026559336488,1.0166782850421827,1.0168570873300702,1.0133959309372158,1.0067771953430888,1.0103834688412185,1.0041425983871153,1.0069830114917453,1.013878310416685,1.0186315219785724,1.0132710740588453,1.0207568075689852,1.0191071478241773,1.016479222190047,1.0054143110172231,0.9927172599946105,0.9631563039768206,0.9756937263621089,0.976441962374194,0.994762262933215,0.9881803124694816,0.9760085772476247,0.9907772347757092,0.9617546422813407,0.9641246634964737,0.9780332460813522,0.9898731847660052,0.982038608343735,0.9901372830014966,0.9896361992346404,0.9639587650585846,0.974545912560365,0.9714242658642347,0.977782704028637,0.9901881186022634,0.9908245989725447,1.0108420789476402,1.0239929461199893,1.0249253993956144,1.0248290459948386,1.0251593528807763,1.0325709281479636,1.0355438576901401,1.0347936951157761,1.0315490096821769,1.0342122137020153,1.0345666298412286,1.0345872951612392,1.0295223937932652,1.0294225961151402,1.0207585935184373,1.027045008364364,1.0245504018928016,1.0191035308549894,1.024247564581078,0.9820967968710144,0.9899259869058753,1.0039996632939374,0.9995034568489072,0.9839503644247936,0.992908732930873,0.9992789330839211,1.010210022957231,1.0088087414335511,1.018852115296318,1.016814876626137,1.0196241649519595,1.0156279061304307,1.0226069113170644,1.0189575682339933,1.0218586922880701,1.0238211169118265,1.0279908337726376,1.0337283979253455,1.0328679170138584,1.0362282186888858,1.033095845251253,1.0037040892383409,1.0025139978744835,1.0032182611433682,1.0059571427020124,1.0085330729466828,1.0065538599361377,1.0081287604511808,1.0088460788801936,1.009690526247189,1.0174605875413794,1.0179725256305474,1.0173692809793886,1.0135478544177075,1.0119436592697673,1.0141445716518327,1.0217580505833912,1.0240013923173887,1.0282760085846303,1.024151367378019,0.9933619048889829,0.999643496048875,1.001143212422562,1.0102893806729702,1.0070940158612913,1.0059893002274503,1.0089148760086188,1.017566548682094,1.0176404056482997,1.024914280649169,1.0252579266458848,1.0248147074530016,1.0293846552881123,1.0344747372133245,1.035370740866389,1.035168379503736,1.0404768953951842,1.040512255893812,1.0344972391152558],\"type\":\"scatter\"}],                        {\"legend\":{\"bgcolor\":\"#F5F6F9\",\"font\":{\"color\":\"#4D5663\"}},\"paper_bgcolor\":\"#F5F6F9\",\"plot_bgcolor\":\"#F5F6F9\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"title\":{\"font\":{\"color\":\"#4D5663\"}},\"xaxis\":{\"gridcolor\":\"#E1E5ED\",\"showgrid\":true,\"tickfont\":{\"color\":\"#4D5663\"},\"title\":{\"font\":{\"color\":\"#4D5663\"},\"text\":\"Days\"},\"zerolinecolor\":\"#E1E5ED\"},\"yaxis\":{\"gridcolor\":\"#E1E5ED\",\"showgrid\":true,\"tickfont\":{\"color\":\"#4D5663\"},\"title\":{\"font\":{\"color\":\"#4D5663\"},\"text\":\"Comulative Return\"},\"zerolinecolor\":\"#E1E5ED\"}},                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('23534f6d-8c2d-4a42-9cd4-50e51c0b6324');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing a module for better and more interactive plot\n",
    "import cufflinks as cf\n",
    "cf.set_config_file(offline = True)\n",
    "\n",
    "'''\n",
    "plotting deep nnf, shallow nnf and, 1/n model performance on the test dataset, compare them with\n",
    "index (s&p) for better understanding\n",
    "'''\n",
    "plot_test.iplot(xTitle='Days', yTitle='Comulative Return')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72a5606bcafec1593511b6d198bb0982fb8ea54acb1913d581966686ae52246b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
